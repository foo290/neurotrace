{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"neurotrace","text":"<p>Work in progress</p> <p>Neurotrace is a Python library designed to facilitate the development of AI applications with a focus on memory management, message handling, and integration with various data storage systems. It provides a structured approach to managing conversational data, enabling developers to build intelligent systems that can remember context, emotions, and user interactions.</p>"},{"location":"#core-schema","title":"Core Schema","text":"<p>The <code>neurotrace.core.schema</code> module defines the fundamental data structures used throughout the project.</p>"},{"location":"#message","title":"Message","text":"<p>The core Message class represents a single message in the system:</p> <pre><code>from neurotrace.core.schema import Message, MessageMetadata, EmotionTag\n\nmessage = Message(\n    role=\"user\",           # Can be \"user\", \"ai\", or \"system\"\n    content=\"Hello!\",      # The message text content\n    metadata=MessageMetadata(\n        source=\"chat\",\n        emotions=EmotionTag(sentiment=\"positive\")\n    )\n)\n</code></pre> <p>Key features of Message: - Auto-generated UUID for each message - Automatic timestamp on creation - Type-safe role validation - Rich metadata support via MessageMetadata</p>"},{"location":"#message-components","title":"Message Components","text":""},{"location":"#emotiontag","title":"EmotionTag","text":"<p>Represents the emotional context of a message:</p> <pre><code>from neurotrace.core.schema import EmotionTag\n\nemotion = EmotionTag(\n    sentiment=\"positive\",  # Can be \"positive\", \"neutral\", or \"negative\"\n    intensity=0.8         # Optional float value indicating intensity\n)\n</code></pre>"},{"location":"#messagemetadata","title":"MessageMetadata","text":"<p>Contains additional information and context about a message:</p> <pre><code>from neurotrace.core.schema import MessageMetadata, EmotionTag\n\nmetadata = MessageMetadata(\n    token_count=150,                    # Number of tokens in the message\n    embedding=[0.1, 0.2, 0.3],         # Vector embedding for similarity search\n    source=\"chat\",                      # Source: \"chat\", \"web\", \"api\", or \"system\"\n    tags=[\"important\", \"follow-up\"],    # Custom tags\n    thread_id=\"thread_123\",            # Conversation thread identifier\n    user_id=\"user_456\",               # Associated user identifier\n    related_ids=[\"msg_789\"],          # Related message IDs\n    emotions=EmotionTag(sentiment=\"positive\"),  # Emotional context\n    compressed=False                   # Compression status\n)\n</code></pre> <p>Each field in MessageMetadata is optional and provides specific context: - <code>token_count</code>: Used for tracking token usage - <code>embedding</code>: Vector representation for similarity search - <code>source</code>: Indicates message origin - <code>tags</code>: Custom categorization - <code>thread_id</code>: Groups messages in conversations - <code>user_id</code>: Links messages to users - <code>related_ids</code>: Connects related messages - <code>emotions</code>: Captures emotional context - <code>compressed</code>: Indicates if content is compressed</p>"},{"location":"#adapters-module","title":"Adapters Module","text":"<p>The <code>neurotrace.core.adapters</code> module provides utilities for converting Message objects to and from various database and framework formats.</p>"},{"location":"#vector-database-adapter","title":"Vector Database Adapter","text":"<p>Convert messages to a format suitable for vector database storage:</p> <pre><code>from neurotrace.core.schema import Message, MessageMetadata\nfrom neurotrace.core.adapters.vector_db_adapter import to_vector_record\n\n# Create a message with an embedding\nmessage = Message(\n    content=\"Hello world\",\n    metadata=MessageMetadata(\n        embedding=[0.1, 0.2, 0.3],\n        tags=[\"greeting\"]\n    )\n)\n\n# Convert to vector DB format\nrecord = to_vector_record(message)\n# Results in: {\n#     \"id\": \"&lt;uuid&gt;\",\n#     \"text\": \"Hello world\",\n#     \"embedding\": [0.1, 0.2, 0.3],\n#     \"metadata\": {\"tags\": [\"greeting\"], ...}\n# }\n</code></pre>"},{"location":"#graph-database-adapter","title":"Graph Database Adapter","text":"<p>Convert messages to graph nodes and relationships:</p> <pre><code>from neurotrace.core.adapters.graph_db_adapter import to_graph_node, graph_edges_from_related_ids\nfrom neurotrace.core.schema import Message, MessageMetadata\n\n# Create a message with related IDs\nmessage = Message(\n    content=\"Follow-up response\",\n    metadata=MessageMetadata(\n        related_ids=[\"msg123\"],\n        tags=[\"follow-up\"]\n    )\n)\n\n# Convert to graph node\nnode = to_graph_node(message)\n# Results in: {\n#     \"id\": \"&lt;uuid&gt;\",\n#     \"labels\": [\"Message\"],\n#     \"properties\": {\n#         \"role\": \"user\",\n#         \"content\": \"Follow-up response\",\n#         \"tags\": [\"follow-up\"],\n#         ...\n#     }\n# }\n\n# Generate relationship edges\nedges = graph_edges_from_related_ids(message)\n# Results in: [{\n#     \"from\": \"&lt;current_msg_id&gt;\",\n#     \"to\": \"msg123\",\n#     \"type\": \"RELATED_TO\"\n# }]\n</code></pre>"},{"location":"#langchain-adapter","title":"LangChain Adapter","text":"<p>Convert between neurotrace Messages and LangChain message types:</p> <pre><code>from neurotrace.core.adapters.langchain_adapter import from_langchain_message\nfrom langchain_core.messages import HumanMessage\n\n# Convert from LangChain to neurotrace format\nlc_msg = HumanMessage(content=\"Hey there!\")\nmsg = from_langchain_message(lc_msg)\n# Results in Message(role=\"user\", content=\"Hey there!\")\n</code></pre> <p>Each adapter provides type-safe conversion between neurotrace's Message format and the target system's format, ensuring compatibility with various databases and frameworks.</p>"},{"location":"#memory-management","title":"Memory Management","text":"<p>The <code>neurotrace</code> system implements a sophisticated memory management system inspired by human memory architecture.</p>"},{"location":"#short-term-memory-stm","title":"Short Term Memory (STM)","text":"<p>Located in <code>neurotrace.core.hippocampus.stm</code>, the <code>ShortTermMemory</code> class provides temporary message storage with the following features:</p> <pre><code>from neurotrace.core.hippocampus.stm import ShortTermMemory\n\n# Initialize with token limit\nstm = ShortTermMemory(max_tokens=50)\n\n# Add messages\nstm.append(message)  # automatically handles token budget\n\n# Retrieve all current messages\nmessages = stm.get_messages()\n\n# Clear memory\nstm.clear()\n</code></pre> <p>Key features: - Token-based memory management - Automatic message eviction when token limit is exceeded - Timestamp-based message tracking - Efficient message retrieval</p>"},{"location":"#langchain-integration","title":"LangChain Integration","text":"<p>The <code>NeurotraceMemory</code> class provides seamless integration with LangChain:</p> <pre><code>from neurotrace.core.memory import NeurotraceMemory\n\nmemory = NeurotraceMemory(max_tokens=20)\n\n# Works with LangChain's memory interface\nmemory.save_context({\"input\": \"What's your name?\"}, {\"output\": \"I'm Neurotrace.\"})\nhistory = memory.load_memory_variables({})\n\n# Access chat history\nmessages = history[\"chat_history\"]  # Returns LangChain message format\n</code></pre> <p>Features: - Compatible with LangChain's memory system - Automatic conversion between Neurotrace and LangChain message formats - Preserves all metadata during conversions - Token budget management - Supports message source tracking</p>"},{"location":"#vector-database-integration","title":"Vector Database Integration","text":"<p>The vector database adapter now supports: - Automatic embedding storage and retrieval - Metadata preservation in vector records - Custom tagging for efficient retrieval - UUID-based message tracking</p> <p>Example usage: <pre><code>from neurotrace.core.adapters.vector_db_adapter import to_vector_record\n\nrecord = to_vector_record(message)\n# Creates a vector record with:\n# - Unique ID\n# - Message content\n# - Embeddings\n# - Complete metadata (tags, source, timestamps, etc.)\n</code></pre></p>"},{"location":"reference/documentation/core/constants/","title":"<code>neurotrace.core.constants</code>","text":""},{"location":"reference/documentation/core/constants/#neurotrace.core.constants","title":"<code>neurotrace.core.constants</code>","text":""},{"location":"reference/documentation/core/memory/","title":"<code>neurotrace.core.memory</code>","text":""},{"location":"reference/documentation/core/memory/#neurotrace.core.memory","title":"<code>neurotrace.core.memory</code>","text":""},{"location":"reference/documentation/core/memory/#neurotrace.core.memory.NeurotraceMemory","title":"<code>NeurotraceMemory</code>","text":"<p>               Bases: <code>BaseMemory</code></p> <p>A LangChain-compatible memory implementation using a hybrid memory system.</p> <p>This class implements a memory system that combines short-term memory (STM) and optional long-term memory (LTM) capabilities. It wraps the ShortTermMemory component and integrates with LangChain's memory interface.</p> <p>Attributes:</p> Name Type Description <code>session_id</code> <code>str</code> <p>Unique identifier for the current chat session.</p> <code>_stm</code> <code>ShortTermMemory</code> <p>Short-term memory component with token limit.</p> <code>_ltm</code> <code>LongTermMemory</code> <p>Optional long-term memory adapter for persistence.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to store in short-term memory. Defaults to 2048.</p> <code>2048</code> <code>history</code> <code>BaseChatMessageHistory</code> <p>LangChain chat history for long-term storage. If provided, enables long-term memory. Defaults to None.</p> <code>None</code> <code>session_id</code> <code>str</code> <p>Identifier for the chat session. Defaults to \"default\".</p> <code>'default'</code> Source code in <code>neurotrace/core/memory.py</code> <pre><code>class NeurotraceMemory(BaseMemory):\n    \"\"\"A LangChain-compatible memory implementation using a hybrid memory system.\n\n    This class implements a memory system that combines short-term memory (STM) and\n    optional long-term memory (LTM) capabilities. It wraps the ShortTermMemory\n    component and integrates with LangChain's memory interface.\n\n    Attributes:\n        session_id (str): Unique identifier for the current chat session.\n        _stm (ShortTermMemory): Short-term memory component with token limit.\n        _ltm (LongTermMemory): Optional long-term memory adapter for persistence.\n\n    Args:\n        max_tokens (int, optional): Maximum number of tokens to store in short-term memory.\n            Defaults to 2048.\n        history (BaseChatMessageHistory, optional): LangChain chat history for long-term\n            storage. If provided, enables long-term memory. Defaults to None.\n        session_id (str, optional): Identifier for the chat session. Defaults to \"default\".\n    \"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra=\"allow\",\n    )\n\n    def __init__(self, max_tokens: int = 2048, history: BaseChatMessageHistory = None, session_id: str = \"default\"):\n        super().__init__()\n        self.session_id = session_id\n        self._stm = ShortTermMemory(max_tokens=max_tokens)\n        self._ltm = LongTermMemory(history, session_id=session_id) if history else None\n\n    @property\n    def memory_variables(self) -&gt; List[str]:\n        \"\"\"Gets the list of memory variables used by this memory component.\n\n        Returns:\n            List[str]: List containing \"chat_history\" as the only memory variable.\n        \"\"\"\n        return [\"chat_history\"]\n\n    def load_memory_variables(self, inputs: Dict[str, Any]) -&gt; Dict[str, List[BaseMessage]]:\n        \"\"\"Retrieves the current memory state as LangChain messages.\n\n        Converts all messages in short-term memory to LangChain's message format\n        for compatibility with the LangChain framework.\n\n        Args:\n            inputs (Dict[str, Any]): Input variables (unused in this implementation).\n\n        Returns:\n            Dict[str, List[BaseMessage]]: Dictionary with \"chat_history\" key containing\n                the list of messages in LangChain format.\n        \"\"\"\n        return {\"chat_history\": [msg.to_langchain_message() for msg in self._stm.get_messages()]}\n\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, Any]) -&gt; None:\n        \"\"\"Saves the conversation context to both short-term and long-term memory.\n\n        Creates Message objects from the input and output and stores them in\n        short-term memory. If long-term memory is enabled, also saves to it.\n\n        Args:\n            inputs (Dict[str, Any]): Dictionary containing user input with key \"input\".\n            outputs (Dict[str, Any]): Dictionary containing AI output with key \"output\".\n        \"\"\"\n        user_input = inputs.get(\"input\") or \"\"\n        ai_output = outputs.get(\"output\") or \"\"\n\n        # Build Message objects\n        user_msg = Message(\n            id=\"boom\",\n            role=str(Role.HUMAN),\n            content=user_input,\n            metadata=MessageMetadata(\n                session_id=self.session_id,\n            ),\n        )\n        ai_msg = Message(\n            id=\"boom\",\n            role=str(Role.AI),\n            content=ai_output,\n            metadata=MessageMetadata(\n                session_id=self.session_id,\n            ),\n        )\n\n        # Save in short-term memory\n        self._stm.append(user_msg)\n        self._stm.append(ai_msg)\n\n        if self._ltm:\n            self._ltm.add_message(user_msg)\n            self._ltm.add_message(ai_msg)\n\n    def clear(self, delete_history: bool = False) -&gt; None:\n        \"\"\"Clears the memory state.\n\n        Clears the short-term memory and optionally the long-term memory if specified.\n\n        Args:\n            delete_history (bool, optional): If True, also clears long-term memory\n                if it exists. Defaults to False.\n        \"\"\"\n        self._stm.clear()\n        if self._ltm and delete_history:\n            self._ltm.clear()\n</code></pre>"},{"location":"reference/documentation/core/memory/#neurotrace.core.memory.NeurotraceMemory.memory_variables","title":"<code>memory_variables</code>  <code>property</code>","text":"<p>Gets the list of memory variables used by this memory component.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List containing \"chat_history\" as the only memory variable.</p>"},{"location":"reference/documentation/core/memory/#neurotrace.core.memory.NeurotraceMemory.clear","title":"<code>clear(delete_history=False)</code>","text":"<p>Clears the memory state.</p> <p>Clears the short-term memory and optionally the long-term memory if specified.</p> <p>Parameters:</p> Name Type Description Default <code>delete_history</code> <code>bool</code> <p>If True, also clears long-term memory if it exists. Defaults to False.</p> <code>False</code> Source code in <code>neurotrace/core/memory.py</code> <pre><code>def clear(self, delete_history: bool = False) -&gt; None:\n    \"\"\"Clears the memory state.\n\n    Clears the short-term memory and optionally the long-term memory if specified.\n\n    Args:\n        delete_history (bool, optional): If True, also clears long-term memory\n            if it exists. Defaults to False.\n    \"\"\"\n    self._stm.clear()\n    if self._ltm and delete_history:\n        self._ltm.clear()\n</code></pre>"},{"location":"reference/documentation/core/memory/#neurotrace.core.memory.NeurotraceMemory.load_memory_variables","title":"<code>load_memory_variables(inputs)</code>","text":"<p>Retrieves the current memory state as LangChain messages.</p> <p>Converts all messages in short-term memory to LangChain's message format for compatibility with the LangChain framework.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Any]</code> <p>Input variables (unused in this implementation).</p> required <p>Returns:</p> Type Description <code>Dict[str, List[BaseMessage]]</code> <p>Dict[str, List[BaseMessage]]: Dictionary with \"chat_history\" key containing the list of messages in LangChain format.</p> Source code in <code>neurotrace/core/memory.py</code> <pre><code>def load_memory_variables(self, inputs: Dict[str, Any]) -&gt; Dict[str, List[BaseMessage]]:\n    \"\"\"Retrieves the current memory state as LangChain messages.\n\n    Converts all messages in short-term memory to LangChain's message format\n    for compatibility with the LangChain framework.\n\n    Args:\n        inputs (Dict[str, Any]): Input variables (unused in this implementation).\n\n    Returns:\n        Dict[str, List[BaseMessage]]: Dictionary with \"chat_history\" key containing\n            the list of messages in LangChain format.\n    \"\"\"\n    return {\"chat_history\": [msg.to_langchain_message() for msg in self._stm.get_messages()]}\n</code></pre>"},{"location":"reference/documentation/core/memory/#neurotrace.core.memory.NeurotraceMemory.save_context","title":"<code>save_context(inputs, outputs)</code>","text":"<p>Saves the conversation context to both short-term and long-term memory.</p> <p>Creates Message objects from the input and output and stores them in short-term memory. If long-term memory is enabled, also saves to it.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Any]</code> <p>Dictionary containing user input with key \"input\".</p> required <code>outputs</code> <code>Dict[str, Any]</code> <p>Dictionary containing AI output with key \"output\".</p> required Source code in <code>neurotrace/core/memory.py</code> <pre><code>def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, Any]) -&gt; None:\n    \"\"\"Saves the conversation context to both short-term and long-term memory.\n\n    Creates Message objects from the input and output and stores them in\n    short-term memory. If long-term memory is enabled, also saves to it.\n\n    Args:\n        inputs (Dict[str, Any]): Dictionary containing user input with key \"input\".\n        outputs (Dict[str, Any]): Dictionary containing AI output with key \"output\".\n    \"\"\"\n    user_input = inputs.get(\"input\") or \"\"\n    ai_output = outputs.get(\"output\") or \"\"\n\n    # Build Message objects\n    user_msg = Message(\n        id=\"boom\",\n        role=str(Role.HUMAN),\n        content=user_input,\n        metadata=MessageMetadata(\n            session_id=self.session_id,\n        ),\n    )\n    ai_msg = Message(\n        id=\"boom\",\n        role=str(Role.AI),\n        content=ai_output,\n        metadata=MessageMetadata(\n            session_id=self.session_id,\n        ),\n    )\n\n    # Save in short-term memory\n    self._stm.append(user_msg)\n    self._stm.append(ai_msg)\n\n    if self._ltm:\n        self._ltm.add_message(user_msg)\n        self._ltm.add_message(ai_msg)\n</code></pre>"},{"location":"reference/documentation/core/orchestrator/","title":"<code>neurotrace.core.orchestrator</code>","text":""},{"location":"reference/documentation/core/orchestrator/#neurotrace.core.orchestrator","title":"<code>neurotrace.core.orchestrator</code>","text":"<p>Memory Orchestrator module for neurotrace.</p>"},{"location":"reference/documentation/core/retriever/","title":"<code>neurotrace.core.retriever</code>","text":""},{"location":"reference/documentation/core/retriever/#neurotrace.core.retriever","title":"<code>neurotrace.core.retriever</code>","text":"<p>Memory Retriever module for neurotrace.</p>"},{"location":"reference/documentation/core/schema/","title":"<code>neurotrace.core.schema</code>","text":""},{"location":"reference/documentation/core/schema/#neurotrace.core.schema","title":"<code>neurotrace.core.schema</code>","text":"<p>Schema definitions for neurotrace core components.</p> <p>This module defines the data structures used for managing neuromorphic memory components including messages, metadata, and emotion tags using Pydantic models.</p> Note <p>All models inherit from Pydantic BaseModel for data validation.</p>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.EmotionTag","title":"<code>EmotionTag</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the emotional context and intensity of a message.</p> <p>Attributes:</p> Name Type Description <code>sentiment</code> <code>Optional[Literal['positive', 'neutral', 'negative']]</code> <p>The emotional tone of the message. Defaults to None.</p> <code>intensity</code> <code>Optional[float]</code> <p>A value indicating the strength of the emotion. Defaults to None.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>class EmotionTag(BaseModel):\n    \"\"\"Represents the emotional context and intensity of a message.\n\n    Attributes:\n        sentiment (Optional[Literal[\"positive\", \"neutral\", \"negative\"]]): The emotional\n            tone of the message. Defaults to None.\n        intensity (Optional[float]): A value indicating the strength of the emotion.\n            Defaults to None.\n    \"\"\"\n\n    sentiment: Optional[Literal[\"positive\", \"neutral\", \"negative\"]] = None\n    intensity: Optional[float] = None\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message","title":"<code>Message</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Message represents a single communication in the system. It includes the sender's role, content, timestamp, and metadata. Each message has a unique identifier generated as a UUID.</p> <p>Example Representation: {     \"id\": \"\",                    # unique message ID     \"role\": \"user\" | \"ai\" | \"system\",           # sender role     \"content\": \"string\",                        # message text     \"timestamp\": \"ISO 8601\",                    # message time (UTC)     \"metadata\": {         \"token_count\": 32,                      # optional, for budgeting/compression         \"embedding\": [...],                     # vector representation (optional in-memory or precomputed)         \"source\": \"chat\" | \"web\" | \"api\",       # source of message         \"tags\": [\"finance\", \"personal\"],        # custom tags for search         \"thread_id\": \"conversation_XYZ\",        # optional thread/conversation tracking         \"user_id\": \"abc123\",                    # to associate memory across sessions         \"related_ids\": [\"msg_id_1\", \"msg_id_2\"],# links to other related messages (graph edge)         \"emotions\": {\"sentiment\": \"positive\", \"intensity\": 0.85},  # optional emotion tagging         \"compressed\": False                     # for summarization/compression tracking         \"session_id\": \"default\"                # session identifier for context     } } Source code in <code>neurotrace/core/schema.py</code> <pre><code>class Message(BaseModel):\n    \"\"\"\n    Message represents a single communication in the system.\n    It includes the sender's role, content, timestamp, and metadata.\n    Each message has a unique identifier generated as a UUID.\n\n    Example Representation:\n    {\n        \"id\": \"&lt;uuid4 or hash&gt;\",                    # unique message ID\n        \"role\": \"user\" | \"ai\" | \"system\",           # sender role\n        \"content\": \"string\",                        # message text\n        \"timestamp\": \"ISO 8601\",                    # message time (UTC)\n        \"metadata\": {\n            \"token_count\": 32,                      # optional, for budgeting/compression\n            \"embedding\": [...],                     # vector representation (optional in-memory or precomputed)\n            \"source\": \"chat\" | \"web\" | \"api\",       # source of message\n            \"tags\": [\"finance\", \"personal\"],        # custom tags for search\n            \"thread_id\": \"conversation_XYZ\",        # optional thread/conversation tracking\n            \"user_id\": \"abc123\",                    # to associate memory across sessions\n            \"related_ids\": [\"msg_id_1\", \"msg_id_2\"],# links to other related messages (graph edge)\n            \"emotions\": {\"sentiment\": \"positive\", \"intensity\": 0.85},  # optional emotion tagging\n            \"compressed\": False                     # for summarization/compression tracking\n            \"session_id\": \"default\"                # session identifier for context\n        }\n    }\n    \"\"\"\n\n    id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    role: str\n    content: str\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(UTC))\n    metadata: MessageMetadata = Field(default_factory=MessageMetadata)\n\n    def estimated_token_length(self) -&gt; int:\n        \"\"\"Estimates the number of tokens in the message content.\n\n        Returns:\n            int: The estimated token count, either from metadata or word-based count.\n\n        Note:\n            Currently uses a simple word-splitting approach if token_count is not set\n            in metadata. TODO: Implement a more accurate token counting method.\n        \"\"\"\n        # todo: Implement a more accurate token counting method\n        return self.metadata.token_count or len(self.content.split())\n\n    def to_langchain_message(self) -&gt; Union[HumanMessage, AIMessage]:\n        \"\"\"Converts this Message to a LangChain compatible format.\n\n        Returns:\n            Union[HumanMessage, AIMessage]: A LangChain message object based on the role.\n\n        Raises:\n            ValueError: If the role is neither 'human' nor 'ai'.\n        \"\"\"\n        if Role.from_string(self.role) is Role.HUMAN:\n            return self.to_human_message()\n        elif Role.from_string(self.role) is Role.AI:\n            return self.to_ai_message()\n        else:\n            raise ValueError(f\"Unsupported role: {self.role}. Use 'human' or 'ai'.\")\n\n    def to_human_message(self) -&gt; HumanMessage:\n        \"\"\"Converts this Message to a LangChain HumanMessage format.\n\n        Returns:\n            HumanMessage: A LangChain HumanMessage with the message content and metadata.\n        \"\"\"\n        return HumanMessage(\n            id=self.id, content=self.content, additional_kwargs={\"id\": self.id, \"metadata\": self.metadata.model_dump()}\n        )\n\n    def to_ai_message(self) -&gt; AIMessage:\n        \"\"\"Converts this Message to a LangChain AIMessage format.\n\n        Returns:\n            AIMessage: A LangChain AIMessage with the message content and metadata.\n        \"\"\"\n        return AIMessage(\n            id=self.id, content=self.content, additional_kwargs={\"id\": self.id, \"metadata\": self.metadata.model_dump()}\n        )\n\n    def to_document(self) -&gt; Document:\n        \"\"\"Convert Message to LangChain-compatible Document with safe metadata.\n\n        Converts the current Message instance to a LangChain Document format,\n        ensuring that the metadata is properly serialized and complex types\n        are filtered out.\n\n        Returns:\n            Document: A LangChain Document instance containing the message content\n                and filtered metadata.\n        \"\"\"\n        raw_metadata = self.metadata.model_dump() if self.metadata else {}\n        doc = Document(page_content=self.content, metadata={\"id\": self.id, \"role\": self.role, **raw_metadata})\n        doc = filter_complex_metadata([doc])  # Remove lists, dicts, etc.\n\n        return doc[0]\n\n    @staticmethod\n    def from_document(doc: Document) -&gt; \"Message\":\n        \"\"\"Creates a Message instance from a LangChain Document.\n\n        Extracts content and metadata from a LangChain Document to create\n        a new Message instance. The role is extracted from metadata with\n        a fallback to HUMAN if not specified.\n\n        Args:\n            doc (Document): The LangChain Document to convert from.\n\n        Returns:\n            Message: A new Message instance containing the document's content\n                and metadata.\n        \"\"\"\n        metadata = doc.metadata or {}\n        role_str = metadata.pop(\"role\", Role.HUMAN.value)  # fallback to human\n        return Message(\n            role=Role.from_string(role_str),\n            content=doc.page_content,\n            metadata=MessageMetadata(**metadata) if metadata else None,\n            id=metadata.get(\"id\"),\n        )\n\n    def __eq__(self, other):\n        \"\"\"Checks equality between two Message objects.\n\n        Compares two Message instances for equality based on their role,\n        content, and metadata. The id field is intentionally excluded from\n        the comparison.\n\n        Args:\n            other: Another object to compare with.\n\n        Returns:\n            bool: True if the messages have the same role, content, and metadata\n                (excluding id), False otherwise.\n        \"\"\"\n        if not isinstance(other, Message):\n            return False\n\n        # not comparing id\n        return self.role == other.role and self.content == other.content and self.metadata == other.metadata\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Checks equality between two Message objects.</p> <p>Compares two Message instances for equality based on their role, content, and metadata. The id field is intentionally excluded from the comparison.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <p>Another object to compare with.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the messages have the same role, content, and metadata (excluding id), False otherwise.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def __eq__(self, other):\n    \"\"\"Checks equality between two Message objects.\n\n    Compares two Message instances for equality based on their role,\n    content, and metadata. The id field is intentionally excluded from\n    the comparison.\n\n    Args:\n        other: Another object to compare with.\n\n    Returns:\n        bool: True if the messages have the same role, content, and metadata\n            (excluding id), False otherwise.\n    \"\"\"\n    if not isinstance(other, Message):\n        return False\n\n    # not comparing id\n    return self.role == other.role and self.content == other.content and self.metadata == other.metadata\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.estimated_token_length","title":"<code>estimated_token_length()</code>","text":"<p>Estimates the number of tokens in the message content.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The estimated token count, either from metadata or word-based count.</p> Note <p>Currently uses a simple word-splitting approach if token_count is not set in metadata. TODO: Implement a more accurate token counting method.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def estimated_token_length(self) -&gt; int:\n    \"\"\"Estimates the number of tokens in the message content.\n\n    Returns:\n        int: The estimated token count, either from metadata or word-based count.\n\n    Note:\n        Currently uses a simple word-splitting approach if token_count is not set\n        in metadata. TODO: Implement a more accurate token counting method.\n    \"\"\"\n    # todo: Implement a more accurate token counting method\n    return self.metadata.token_count or len(self.content.split())\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.from_document","title":"<code>from_document(doc)</code>  <code>staticmethod</code>","text":"<p>Creates a Message instance from a LangChain Document.</p> <p>Extracts content and metadata from a LangChain Document to create a new Message instance. The role is extracted from metadata with a fallback to HUMAN if not specified.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Document</code> <p>The LangChain Document to convert from.</p> required <p>Returns:</p> Name Type Description <code>Message</code> <code>Message</code> <p>A new Message instance containing the document's content and metadata.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>@staticmethod\ndef from_document(doc: Document) -&gt; \"Message\":\n    \"\"\"Creates a Message instance from a LangChain Document.\n\n    Extracts content and metadata from a LangChain Document to create\n    a new Message instance. The role is extracted from metadata with\n    a fallback to HUMAN if not specified.\n\n    Args:\n        doc (Document): The LangChain Document to convert from.\n\n    Returns:\n        Message: A new Message instance containing the document's content\n            and metadata.\n    \"\"\"\n    metadata = doc.metadata or {}\n    role_str = metadata.pop(\"role\", Role.HUMAN.value)  # fallback to human\n    return Message(\n        role=Role.from_string(role_str),\n        content=doc.page_content,\n        metadata=MessageMetadata(**metadata) if metadata else None,\n        id=metadata.get(\"id\"),\n    )\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.to_ai_message","title":"<code>to_ai_message()</code>","text":"<p>Converts this Message to a LangChain AIMessage format.</p> <p>Returns:</p> Name Type Description <code>AIMessage</code> <code>AIMessage</code> <p>A LangChain AIMessage with the message content and metadata.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def to_ai_message(self) -&gt; AIMessage:\n    \"\"\"Converts this Message to a LangChain AIMessage format.\n\n    Returns:\n        AIMessage: A LangChain AIMessage with the message content and metadata.\n    \"\"\"\n    return AIMessage(\n        id=self.id, content=self.content, additional_kwargs={\"id\": self.id, \"metadata\": self.metadata.model_dump()}\n    )\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.to_document","title":"<code>to_document()</code>","text":"<p>Convert Message to LangChain-compatible Document with safe metadata.</p> <p>Converts the current Message instance to a LangChain Document format, ensuring that the metadata is properly serialized and complex types are filtered out.</p> <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>A LangChain Document instance containing the message content and filtered metadata.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def to_document(self) -&gt; Document:\n    \"\"\"Convert Message to LangChain-compatible Document with safe metadata.\n\n    Converts the current Message instance to a LangChain Document format,\n    ensuring that the metadata is properly serialized and complex types\n    are filtered out.\n\n    Returns:\n        Document: A LangChain Document instance containing the message content\n            and filtered metadata.\n    \"\"\"\n    raw_metadata = self.metadata.model_dump() if self.metadata else {}\n    doc = Document(page_content=self.content, metadata={\"id\": self.id, \"role\": self.role, **raw_metadata})\n    doc = filter_complex_metadata([doc])  # Remove lists, dicts, etc.\n\n    return doc[0]\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.to_human_message","title":"<code>to_human_message()</code>","text":"<p>Converts this Message to a LangChain HumanMessage format.</p> <p>Returns:</p> Name Type Description <code>HumanMessage</code> <code>HumanMessage</code> <p>A LangChain HumanMessage with the message content and metadata.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def to_human_message(self) -&gt; HumanMessage:\n    \"\"\"Converts this Message to a LangChain HumanMessage format.\n\n    Returns:\n        HumanMessage: A LangChain HumanMessage with the message content and metadata.\n    \"\"\"\n    return HumanMessage(\n        id=self.id, content=self.content, additional_kwargs={\"id\": self.id, \"metadata\": self.metadata.model_dump()}\n    )\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.to_langchain_message","title":"<code>to_langchain_message()</code>","text":"<p>Converts this Message to a LangChain compatible format.</p> <p>Returns:</p> Type Description <code>Union[HumanMessage, AIMessage]</code> <p>Union[HumanMessage, AIMessage]: A LangChain message object based on the role.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the role is neither 'human' nor 'ai'.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def to_langchain_message(self) -&gt; Union[HumanMessage, AIMessage]:\n    \"\"\"Converts this Message to a LangChain compatible format.\n\n    Returns:\n        Union[HumanMessage, AIMessage]: A LangChain message object based on the role.\n\n    Raises:\n        ValueError: If the role is neither 'human' nor 'ai'.\n    \"\"\"\n    if Role.from_string(self.role) is Role.HUMAN:\n        return self.to_human_message()\n    elif Role.from_string(self.role) is Role.AI:\n        return self.to_ai_message()\n    else:\n        raise ValueError(f\"Unsupported role: {self.role}. Use 'human' or 'ai'.\")\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.MessageMetadata","title":"<code>MessageMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Contains additional contextual information about a message.</p> <p>Attributes:</p> Name Type Description <code>token_count</code> <code>Optional[int]</code> <p>Number of tokens in the associated message.</p> <code>embedding</code> <code>Optional[List[float]]</code> <p>Vector representation of the message content.</p> <code>source</code> <code>Optional[Literal['chat', 'web', 'api', 'system']]</code> <p>Origin of the message. Defaults to \"chat\".</p> <code>tags</code> <code>Optional[List[str]]</code> <p>List of categorical tags. Defaults to empty list.</p> <code>thread_id</code> <code>Optional[str]</code> <p>Unique identifier for the conversation thread.</p> <code>user_id</code> <code>Optional[str]</code> <p>Identifier for the message author.</p> <code>related_ids</code> <code>Optional[List[str]]</code> <p>References to related message IDs.</p> <code>emotions</code> <code>Optional[EmotionTag]</code> <p>Emotional analysis of the message.</p> <code>compressed</code> <code>Optional[bool]</code> <p>Whether the message content is compressed. Defaults to False.</p> <code>session_id</code> <code>Optional[str]</code> <p>Current session identifier. Defaults to 'default'.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>class MessageMetadata(BaseModel):\n    \"\"\"Contains additional contextual information about a message.\n\n    Attributes:\n        token_count (Optional[int]): Number of tokens in the associated message.\n        embedding (Optional[List[float]]): Vector representation of the message content.\n        source (Optional[Literal[\"chat\", \"web\", \"api\", \"system\"]]): Origin of the message.\n            Defaults to \"chat\".\n        tags (Optional[List[str]]): List of categorical tags. Defaults to empty list.\n        thread_id (Optional[str]): Unique identifier for the conversation thread.\n        user_id (Optional[str]): Identifier for the message author.\n        related_ids (Optional[List[str]]): References to related message IDs.\n        emotions (Optional[EmotionTag]): Emotional analysis of the message.\n        compressed (Optional[bool]): Whether the message content is compressed.\n            Defaults to False.\n        session_id (Optional[str]): Current session identifier. Defaults to 'default'.\n    \"\"\"\n\n    token_count: Optional[int] = None\n    embedding: Optional[List[float]] = None\n    source: Optional[Literal[\"chat\", \"web\", \"api\", \"system\"]] = \"chat\"\n    tags: Optional[List[str]] = []\n    thread_id: Optional[str] = None\n    user_id: Optional[str] = None\n    related_ids: Optional[List[str]] = []\n    emotions: Optional[EmotionTag] = None\n    compressed: Optional[bool] = False\n    session_id: Optional[str] = \"default\"\n</code></pre>"},{"location":"reference/documentation/core/adapters/graph_db_adapter/","title":"<code>neurotrace.core.adapters.graph_db_adapter</code>","text":""},{"location":"reference/documentation/core/adapters/graph_db_adapter/#neurotrace.core.adapters.graph_db_adapter","title":"<code>neurotrace.core.adapters.graph_db_adapter</code>","text":"<p>Graph Database Adapter Module.</p> <p>This module provides adapter functions for converting neurotrace Message objects into graph database compatible formats. It handles the transformation of messages into graph nodes and relationship edges, suitable for graph database storage and querying.</p>"},{"location":"reference/documentation/core/adapters/graph_db_adapter/#neurotrace.core.adapters.graph_db_adapter.graph_edges_from_related_ids","title":"<code>graph_edges_from_related_ids(msg)</code>","text":"<p>Generate graph relationship edges from a message's related IDs.</p> <p>This function creates relationship edges between the given message and its related messages, as specified in the message's metadata. Each relationship is of type \"RELATED_TO\".</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Message</code> <p>The Message object to generate relationship edges for.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of edge dictionaries, each containing: - from: The source message ID (current message) - to: The target message ID (related message) - type: The relationship type (\"RELATED_TO\")</p> Example <p>msg = Message(id=\"123\", metadata=MessageMetadata(related_ids=[\"456\"])) edges = graph_edges_from_related_ids(msg) print(edges[0])  # {\"from\": \"123\", \"to\": \"456\", \"type\": \"RELATED_TO\"}</p> Source code in <code>neurotrace/core/adapters/graph_db_adapter.py</code> <pre><code>def graph_edges_from_related_ids(msg: \"Message\") -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Generate graph relationship edges from a message's related IDs.\n\n    This function creates relationship edges between the given message and its\n    related messages, as specified in the message's metadata. Each relationship\n    is of type \"RELATED_TO\".\n\n    Args:\n        msg (Message): The Message object to generate relationship edges for.\n\n    Returns:\n        List[Dict[str, str]]: A list of edge dictionaries, each containing:\n            - from: The source message ID (current message)\n            - to: The target message ID (related message)\n            - type: The relationship type (\"RELATED_TO\")\n\n    Example:\n        &gt;&gt;&gt; msg = Message(id=\"123\", metadata=MessageMetadata(related_ids=[\"456\"]))\n        &gt;&gt;&gt; edges = graph_edges_from_related_ids(msg)\n        &gt;&gt;&gt; print(edges[0])  # {\"from\": \"123\", \"to\": \"456\", \"type\": \"RELATED_TO\"}\n    \"\"\"\n    return [{\"from\": msg.id, \"to\": rid, \"type\": \"RELATED_TO\"} for rid in msg.metadata.related_ids or []]\n</code></pre>"},{"location":"reference/documentation/core/adapters/graph_db_adapter/#neurotrace.core.adapters.graph_db_adapter.to_graph_node","title":"<code>to_graph_node(msg)</code>","text":"<p>Convert a Message object to a graph database node format.</p> <p>This function transforms a neurotrace Message into a dictionary format suitable for creating nodes in a graph database. It includes the message's core properties and flattens metadata into the node properties.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Message</code> <p>The Message object to convert into a graph node.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing: - id: The unique identifier for the node - labels: List of node labels ([\"Message\"]) - properties: Dict containing:     - role: The message role (user/ai/system)     - content: The message text content     - timestamp: ISO formatted timestamp     - Additional properties from message metadata</p> Example <p>msg = Message(id=\"123\", content=\"Hello\", role=\"user\") node = to_graph_node(msg) print(node[\"labels\"])  # [\"Message\"] print(node[\"properties\"][\"role\"])  # \"user\"</p> Source code in <code>neurotrace/core/adapters/graph_db_adapter.py</code> <pre><code>def to_graph_node(msg: \"Message\") -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert a Message object to a graph database node format.\n\n    This function transforms a neurotrace Message into a dictionary format\n    suitable for creating nodes in a graph database. It includes the message's\n    core properties and flattens metadata into the node properties.\n\n    Args:\n        msg (Message): The Message object to convert into a graph node.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing:\n            - id: The unique identifier for the node\n            - labels: List of node labels ([\"Message\"])\n            - properties: Dict containing:\n                - role: The message role (user/ai/system)\n                - content: The message text content\n                - timestamp: ISO formatted timestamp\n                - Additional properties from message metadata\n\n    Example:\n        &gt;&gt;&gt; msg = Message(id=\"123\", content=\"Hello\", role=\"user\")\n        &gt;&gt;&gt; node = to_graph_node(msg)\n        &gt;&gt;&gt; print(node[\"labels\"])  # [\"Message\"]\n        &gt;&gt;&gt; print(node[\"properties\"][\"role\"])  # \"user\"\n    \"\"\"\n    return {\n        \"id\": msg.id,\n        \"labels\": [\"Message\"],\n        \"properties\": {\n            \"role\": msg.role,\n            \"content\": msg.content,\n            \"timestamp\": msg.timestamp.isoformat(),\n            **msg.metadata.model_dump()\n        }\n    }\n</code></pre>"},{"location":"reference/documentation/core/adapters/langchain_adapter/","title":"<code>neurotrace.core.adapters.langchain_adapter</code>","text":""},{"location":"reference/documentation/core/adapters/langchain_adapter/#neurotrace.core.adapters.langchain_adapter","title":"<code>neurotrace.core.adapters.langchain_adapter</code>","text":"<p>LangChain Adapter Module.</p> <p>This module provides adapter functions for converting between neurotrace's internal Message format and LangChain's message formats. It handles bidirectional conversion between neurotrace Messages and LangChain's HumanMessage/AIMessage types.</p>"},{"location":"reference/documentation/core/adapters/langchain_adapter/#neurotrace.core.adapters.langchain_adapter.from_langchain_message","title":"<code>from_langchain_message(msg, role=None)</code>","text":"<p>Convert a LangChain message to a neurotrace Message.</p> <p>This function transforms a LangChain message into neurotrace's Message format, with automatic role detection based on the message type or an optional explicit role override.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>BaseMessage</code> <p>The LangChain message to convert.</p> required <code>role</code> <code>Optional[str]</code> <p>Explicitly specify the role to use. If None, role is detected from the message type. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Message</code> <code>Message</code> <p>A neurotrace Message with: - role determined by message type or override - content from the original message</p> Example <p>lc_msg = HumanMessage(content=\"Hello\") msg = from_langchain_message(lc_msg) print(msg.role)  # \"user\" print(msg.content)  # \"Hello\"</p> Source code in <code>neurotrace/core/adapters/langchain_adapter.py</code> <pre><code>def from_langchain_message(msg: BaseMessage, role: Optional[str] = None) -&gt; Message:\n    \"\"\"\n    Convert a LangChain message to a neurotrace Message.\n\n    This function transforms a LangChain message into neurotrace's Message format,\n    with automatic role detection based on the message type or an optional\n    explicit role override.\n\n    Args:\n        msg (BaseMessage): The LangChain message to convert.\n        role (Optional[str], optional): Explicitly specify the role to use.\n            If None, role is detected from the message type. Defaults to None.\n\n    Returns:\n        Message: A neurotrace Message with:\n            - role determined by message type or override\n            - content from the original message\n\n    Example:\n        &gt;&gt;&gt; lc_msg = HumanMessage(content=\"Hello\")\n        &gt;&gt;&gt; msg = from_langchain_message(lc_msg)\n        &gt;&gt;&gt; print(msg.role)  # \"user\"\n        &gt;&gt;&gt; print(msg.content)  # \"Hello\"\n    \"\"\"\n    detected_role = (\n        role if role else\n        \"human\" if isinstance(msg, HumanMessage) else\n        \"ai\" if isinstance(msg, AIMessage) else\n        \"system\"\n    )\n\n    role_literal = cast(Literal[\"user\", \"ai\", \"system\"], detected_role)\n    return Message(\n        role=role_literal,\n        content=msg.content\n    )\n</code></pre>"},{"location":"reference/documentation/core/adapters/vector_db_adapter/","title":"<code>neurotrace.core.adapters.vector_db_adapter</code>","text":""},{"location":"reference/documentation/core/adapters/vector_db_adapter/#neurotrace.core.adapters.vector_db_adapter","title":"<code>neurotrace.core.adapters.vector_db_adapter</code>","text":"<p>Vector Database Adapter Module.</p> <p>This module provides adapter functions for converting between neurotrace's internal message format and vector database record format. It handles the serialization of Message objects into a format suitable for vector database storage and retrieval.</p>"},{"location":"reference/documentation/core/adapters/vector_db_adapter/#neurotrace.core.adapters.vector_db_adapter.to_vector_record","title":"<code>to_vector_record(msg)</code>","text":"<p>Converts a Message object to a vector database record format.</p> <p>This function transforms a neurotrace Message instance into a dictionary format suitable for storage in a vector database. It extracts essential fields including the message ID, content text, embedding vector, and all associated metadata.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Message</code> <p>The Message object to convert.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing: - id: The message's unique identifier - text: The message content - embedding: The message's vector embedding - metadata: All additional metadata as a dictionary</p> Example <p>message = Message(id=\"123\", content=\"Hello\", metadata=MessageMetadata(...)) record = to_vector_record(message) print(record) {     'id': '123',     'text': 'Hello',     'embedding': [...],     'metadata': {...} }</p> Source code in <code>neurotrace/core/adapters/vector_db_adapter.py</code> <pre><code>def to_vector_record(msg: Message) -&gt; Dict[str, Any]:\n    \"\"\"\n    Converts a Message object to a vector database record format.\n\n    This function transforms a neurotrace Message instance into a dictionary format\n    suitable for storage in a vector database. It extracts essential fields including\n    the message ID, content text, embedding vector, and all associated metadata.\n\n    Args:\n        msg (Message): The Message object to convert.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing:\n            - id: The message's unique identifier\n            - text: The message content\n            - embedding: The message's vector embedding\n            - metadata: All additional metadata as a dictionary\n\n    Example:\n        &gt;&gt;&gt; message = Message(id=\"123\", content=\"Hello\", metadata=MessageMetadata(...))\n        &gt;&gt;&gt; record = to_vector_record(message)\n        &gt;&gt;&gt; print(record)\n        {\n            'id': '123',\n            'text': 'Hello',\n            'embedding': [...],\n            'metadata': {...}\n        }\n    \"\"\"\n    return {\n        \"id\": msg.id,\n        \"text\": msg.content,\n        \"embedding\": msg.metadata.embedding,\n        \"metadata\": msg.metadata.model_dump()\n    }\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/","title":"<code>neurotrace.core.hippocampus.ltm</code>","text":""},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm","title":"<code>neurotrace.core.hippocampus.ltm</code>","text":""},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.BaseLongTermMemory","title":"<code>BaseLongTermMemory</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for long-term memory storage.</p> <p>This class defines the interface for persistent storage of conversation messages. Implementations should handle the storage and retrieval of messages across different chat sessions.</p> Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>class BaseLongTermMemory(ABC):\n    \"\"\"Abstract base class for long-term memory storage.\n\n    This class defines the interface for persistent storage of conversation\n    messages. Implementations should handle the storage and retrieval of\n    messages across different chat sessions.\n    \"\"\"\n\n    @abstractmethod\n    def add_message(self, message: Message) -&gt; None:\n        \"\"\"Store a message in long-term memory.\n\n        Args:\n            message (Message): The message object to be stored.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_user_message(self, content: str) -&gt; None:\n        \"\"\"Add a user message to long-term memory.\n\n        Args:\n            content (str): The content of the user's message.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_ai_message(self, content: str) -&gt; None:\n        \"\"\"Add an AI message to long-term memory.\n\n        Args:\n            content (str): The content of the AI's message.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_messages(self, session_id: str) -&gt; List[Message]:\n        \"\"\"Retrieve all messages for a given session.\n\n        Args:\n            session_id (str): The identifier for the chat session.\n\n        Returns:\n            List[Message]: List of messages associated with the session.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def clear(self, session_id: str) -&gt; None:\n        \"\"\"Clear messages for a given session.\n\n        Args:\n            session_id (str): The identifier for the chat session to clear.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.BaseLongTermMemory.add_ai_message","title":"<code>add_ai_message(content)</code>  <code>abstractmethod</code>","text":"<p>Add an AI message to long-term memory.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the AI's message.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>@abstractmethod\ndef add_ai_message(self, content: str) -&gt; None:\n    \"\"\"Add an AI message to long-term memory.\n\n    Args:\n        content (str): The content of the AI's message.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.BaseLongTermMemory.add_message","title":"<code>add_message(message)</code>  <code>abstractmethod</code>","text":"<p>Store a message in long-term memory.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The message object to be stored.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>@abstractmethod\ndef add_message(self, message: Message) -&gt; None:\n    \"\"\"Store a message in long-term memory.\n\n    Args:\n        message (Message): The message object to be stored.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.BaseLongTermMemory.add_user_message","title":"<code>add_user_message(content)</code>  <code>abstractmethod</code>","text":"<p>Add a user message to long-term memory.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the user's message.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>@abstractmethod\ndef add_user_message(self, content: str) -&gt; None:\n    \"\"\"Add a user message to long-term memory.\n\n    Args:\n        content (str): The content of the user's message.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.BaseLongTermMemory.clear","title":"<code>clear(session_id)</code>  <code>abstractmethod</code>","text":"<p>Clear messages for a given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The identifier for the chat session to clear.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>@abstractmethod\ndef clear(self, session_id: str) -&gt; None:\n    \"\"\"Clear messages for a given session.\n\n    Args:\n        session_id (str): The identifier for the chat session to clear.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.BaseLongTermMemory.get_messages","title":"<code>get_messages(session_id)</code>  <code>abstractmethod</code>","text":"<p>Retrieve all messages for a given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The identifier for the chat session.</p> required <p>Returns:</p> Type Description <code>List[Message]</code> <p>List[Message]: List of messages associated with the session.</p> Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>@abstractmethod\ndef get_messages(self, session_id: str) -&gt; List[Message]:\n    \"\"\"Retrieve all messages for a given session.\n\n    Args:\n        session_id (str): The identifier for the chat session.\n\n    Returns:\n        List[Message]: List of messages associated with the session.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory","title":"<code>LongTermMemory</code>","text":"<p>               Bases: <code>BaseLongTermMemory</code></p> <p>LangChain chat history adapter for long-term memory storage.</p> <p>This adapter implements the BaseLongTermMemory interface using LangChain's chat history components for persistent storage.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>BaseChatMessageHistory</code> <p>LangChain chat history implementation to use for storage.</p> required <code>session_id</code> <code>str</code> <p>Default session identifier. Defaults to \"default\".</p> <code>'default'</code> Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>class LongTermMemory(BaseLongTermMemory):\n    \"\"\"LangChain chat history adapter for long-term memory storage.\n\n    This adapter implements the BaseLongTermMemory interface using LangChain's\n    chat history components for persistent storage.\n\n    Args:\n        history (BaseChatMessageHistory): LangChain chat history implementation\n            to use for storage.\n        session_id (str, optional): Default session identifier. Defaults to \"default\".\n    \"\"\"\n\n    def __init__(self, history: BaseChatMessageHistory, session_id: str = \"default\"):\n        \"\"\"Initialize the LangChain history adapter.\n\n        Args:\n            history (BaseChatMessageHistory): LangChain chat history implementation.\n            session_id (str, optional): Default session identifier. Defaults to \"default\".\n        \"\"\"\n        self.history = history\n        self.session_id = session_id\n\n    def add_message(self, message: Message) -&gt; None:\n        \"\"\"Add a message to the LangChain chat history.\n\n        Converts the Message object to LangChain's message format before storing.\n\n        Args:\n            message (Message): The message to store.\n        \"\"\"\n        lc_msg: BaseMessage = message.to_langchain_message()\n        self.history.add_message(lc_msg)\n\n    def add_user_message(self, content: str) -&gt; None:\n        \"\"\"Add a user message to the chat history.\n\n        Creates a Message object with HUMAN role and adds it to storage.\n\n        Args:\n            content (str): The content of the user's message.\n        \"\"\"\n        self.add_message(Message(role=Role.HUMAN.value, content=content))\n\n    def add_ai_message(self, content: str) -&gt; None:\n        \"\"\"Add an AI message to the chat history.\n\n        Creates a Message object with AI role and adds it to storage.\n\n        Args:\n            content (str): The content of the AI's message.\n        \"\"\"\n        self.add_message(Message(role=Role.AI.value, content=content))\n\n    def get_messages(self, session_id: str = None) -&gt; List[Message]:\n        \"\"\"Retrieve all messages from the chat history.\n\n        Args:\n            session_id (str, optional): Session identifier. Currently unused as\n                LangChain history doesn't support session filtering. Defaults to None.\n\n        Returns:\n            List[Message]: All messages in the chat history.\n        \"\"\"\n        lc_msgs = self.history.messages\n        return [from_langchain_message(m) for m in lc_msgs]\n\n    def clear(self, session_id: str = None) -&gt; None:\n        \"\"\"Clear all messages from the chat history.\n\n        Args:\n            session_id (str, optional): Session identifier. Currently unused as\n                LangChain history doesn't support session filtering. Defaults to None.\n        \"\"\"\n        self.history.clear()\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory.__init__","title":"<code>__init__(history, session_id='default')</code>","text":"<p>Initialize the LangChain history adapter.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>BaseChatMessageHistory</code> <p>LangChain chat history implementation.</p> required <code>session_id</code> <code>str</code> <p>Default session identifier. Defaults to \"default\".</p> <code>'default'</code> Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>def __init__(self, history: BaseChatMessageHistory, session_id: str = \"default\"):\n    \"\"\"Initialize the LangChain history adapter.\n\n    Args:\n        history (BaseChatMessageHistory): LangChain chat history implementation.\n        session_id (str, optional): Default session identifier. Defaults to \"default\".\n    \"\"\"\n    self.history = history\n    self.session_id = session_id\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory.add_ai_message","title":"<code>add_ai_message(content)</code>","text":"<p>Add an AI message to the chat history.</p> <p>Creates a Message object with AI role and adds it to storage.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the AI's message.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>def add_ai_message(self, content: str) -&gt; None:\n    \"\"\"Add an AI message to the chat history.\n\n    Creates a Message object with AI role and adds it to storage.\n\n    Args:\n        content (str): The content of the AI's message.\n    \"\"\"\n    self.add_message(Message(role=Role.AI.value, content=content))\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory.add_message","title":"<code>add_message(message)</code>","text":"<p>Add a message to the LangChain chat history.</p> <p>Converts the Message object to LangChain's message format before storing.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The message to store.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>def add_message(self, message: Message) -&gt; None:\n    \"\"\"Add a message to the LangChain chat history.\n\n    Converts the Message object to LangChain's message format before storing.\n\n    Args:\n        message (Message): The message to store.\n    \"\"\"\n    lc_msg: BaseMessage = message.to_langchain_message()\n    self.history.add_message(lc_msg)\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory.add_user_message","title":"<code>add_user_message(content)</code>","text":"<p>Add a user message to the chat history.</p> <p>Creates a Message object with HUMAN role and adds it to storage.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the user's message.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>def add_user_message(self, content: str) -&gt; None:\n    \"\"\"Add a user message to the chat history.\n\n    Creates a Message object with HUMAN role and adds it to storage.\n\n    Args:\n        content (str): The content of the user's message.\n    \"\"\"\n    self.add_message(Message(role=Role.HUMAN.value, content=content))\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory.clear","title":"<code>clear(session_id=None)</code>","text":"<p>Clear all messages from the chat history.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Session identifier. Currently unused as LangChain history doesn't support session filtering. Defaults to None.</p> <code>None</code> Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>def clear(self, session_id: str = None) -&gt; None:\n    \"\"\"Clear all messages from the chat history.\n\n    Args:\n        session_id (str, optional): Session identifier. Currently unused as\n            LangChain history doesn't support session filtering. Defaults to None.\n    \"\"\"\n    self.history.clear()\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory.get_messages","title":"<code>get_messages(session_id=None)</code>","text":"<p>Retrieve all messages from the chat history.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Session identifier. Currently unused as LangChain history doesn't support session filtering. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>List[Message]: All messages in the chat history.</p> Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>def get_messages(self, session_id: str = None) -&gt; List[Message]:\n    \"\"\"Retrieve all messages from the chat history.\n\n    Args:\n        session_id (str, optional): Session identifier. Currently unused as\n            LangChain history doesn't support session filtering. Defaults to None.\n\n    Returns:\n        List[Message]: All messages in the chat history.\n    \"\"\"\n    lc_msgs = self.history.messages\n    return [from_langchain_message(m) for m in lc_msgs]\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/","title":"<code>neurotrace.core.hippocampus.stm</code>","text":""},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm","title":"<code>neurotrace.core.hippocampus.stm</code>","text":""},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory","title":"<code>BaseShortTermMemory</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for short-term memory management.</p> <p>This class defines the interface for managing a temporary message store with token limit constraints.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>class BaseShortTermMemory(ABC):\n    \"\"\"Abstract base class for short-term memory management.\n\n    This class defines the interface for managing a temporary message store\n    with token limit constraints.\n    \"\"\"\n\n    @abstractmethod\n    def append(self, message: Message) -&gt; None:\n        \"\"\"Add a message to short-term memory.\n\n        Args:\n            message (Message): The message to be added to memory.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_messages(self) -&gt; List[Message]:\n        \"\"\"Retrieve all messages from short-term memory.\n\n        Returns:\n            List[Message]: List of all messages currently in memory.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"Remove all messages from short-term memory.\"\"\"\n        ...\n\n    @abstractmethod\n    def set_messages(self, messages: List[Message]) -&gt; None:\n        \"\"\"Replace all messages in memory with a new list.\n\n        Args:\n            messages (List[Message]): New list of messages to store.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def total_tokens(self) -&gt; int:\n        \"\"\"Calculate total tokens used by all messages.\n\n        Returns:\n            int: Sum of estimated token lengths of all messages.\n        \"\"\"\n        ...\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the number of messages in memory.\n\n        Returns:\n            int: Count of messages currently stored.\n        \"\"\"\n        return len(self.get_messages())\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of messages in memory.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Count of messages currently stored.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of messages in memory.\n\n    Returns:\n        int: Count of messages currently stored.\n    \"\"\"\n    return len(self.get_messages())\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory.append","title":"<code>append(message)</code>  <code>abstractmethod</code>","text":"<p>Add a message to short-term memory.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The message to be added to memory.</p> required Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>@abstractmethod\ndef append(self, message: Message) -&gt; None:\n    \"\"\"Add a message to short-term memory.\n\n    Args:\n        message (Message): The message to be added to memory.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>Remove all messages from short-term memory.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"Remove all messages from short-term memory.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory.get_messages","title":"<code>get_messages()</code>  <code>abstractmethod</code>","text":"<p>Retrieve all messages from short-term memory.</p> <p>Returns:</p> Type Description <code>List[Message]</code> <p>List[Message]: List of all messages currently in memory.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>@abstractmethod\ndef get_messages(self) -&gt; List[Message]:\n    \"\"\"Retrieve all messages from short-term memory.\n\n    Returns:\n        List[Message]: List of all messages currently in memory.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory.set_messages","title":"<code>set_messages(messages)</code>  <code>abstractmethod</code>","text":"<p>Replace all messages in memory with a new list.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>New list of messages to store.</p> required Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>@abstractmethod\ndef set_messages(self, messages: List[Message]) -&gt; None:\n    \"\"\"Replace all messages in memory with a new list.\n\n    Args:\n        messages (List[Message]): New list of messages to store.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory.total_tokens","title":"<code>total_tokens()</code>  <code>abstractmethod</code>","text":"<p>Calculate total tokens used by all messages.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Sum of estimated token lengths of all messages.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>@abstractmethod\ndef total_tokens(self) -&gt; int:\n    \"\"\"Calculate total tokens used by all messages.\n\n    Returns:\n        int: Sum of estimated token lengths of all messages.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory","title":"<code>ShortTermMemory</code>","text":"<p>               Bases: <code>BaseShortTermMemory</code></p> <p>Implementation of token-limited short-term memory.</p> <p>This class maintains a list of messages while ensuring the total token count stays within a specified limit. When the limit is exceeded, older messages are automatically evicted.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to store. Set to 0 to disable memory (all messages will be evicted). Defaults to 2048.</p> <code>2048</code> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>class ShortTermMemory(BaseShortTermMemory):\n    \"\"\"Implementation of token-limited short-term memory.\n\n    This class maintains a list of messages while ensuring the total token\n    count stays within a specified limit. When the limit is exceeded, older\n    messages are automatically evicted.\n\n    Args:\n        max_tokens (int, optional): Maximum number of tokens to store. Set to 0\n            to disable memory (all messages will be evicted). Defaults to 2048.\n    \"\"\"\n\n    def __init__(self, max_tokens: int = 2048):\n        \"\"\"Initialize short-term memory with token limit.\n\n        Args:\n            max_tokens (int, optional): Maximum number of tokens to store.\n                Defaults to 2048.\n        \"\"\"\n        self.messages: List[Message] = []\n        self.max_tokens = max_tokens\n\n    def append(self, message: Message) -&gt; None:\n        \"\"\"Add a message to memory and evict old messages if needed.\n\n        If the message doesn't have an ID, generates a UUID for it. After\n        adding the message, ensures token limit compliance by evicting old\n        messages if necessary.\n\n        Args:\n            message (Message): The message to add to memory.\n        \"\"\"\n        if not message.id:\n            message.id = str(uuid.uuid4())\n\n        self.messages.append(message)\n        self._evict_if_needed()\n\n    def get_messages(self) -&gt; List[Message]:\n        \"\"\"Get all messages currently in memory.\n\n        Returns:\n            List[Message]: List of all stored messages.\n        \"\"\"\n        return self.messages\n\n    def clear(self) -&gt; None:\n        \"\"\"Remove all messages from memory.\"\"\"\n        self.messages = []\n\n    def _evict_if_needed(self) -&gt; None:\n        \"\"\"Maintain token limit by removing oldest messages.\n\n        If max_tokens is 0, clears all messages. Otherwise, removes oldest\n        messages until total token count is within limit, always keeping at\n        least one message.\n        \"\"\"\n        # If max_tokens is 0, clear everything (user wants no memory)\n        if self.max_tokens == 0:\n            self.messages.clear()\n            return\n\n        total = sum(msg.estimated_token_length() for msg in self.messages)\n\n        # Keep at least 1 message even if over limit (unless max_tokens is zero)\n        while total &gt; self.max_tokens and len(self.messages) &gt; 1:\n            total -= self.messages[0].estimated_token_length()\n            self.messages.pop(0)\n\n    def set_messages(self, messages: List[Message]) -&gt; None:\n        \"\"\"Replace current messages with new list and maintain token limit.\n\n        Args:\n            messages (List[Message]): New messages to store in memory.\n        \"\"\"\n        self.messages = messages\n        self._evict_if_needed()\n\n    def total_tokens(self) -&gt; int:\n        \"\"\"Calculate total tokens used by all messages.\n\n        Returns:\n            int: Sum of estimated token lengths across all messages.\n        \"\"\"\n        return sum(m.estimated_token_length() for m in self.messages)\n\n    def __len__(self):\n        \"\"\"Get number of messages in memory.\n\n        Returns:\n            int: Count of stored messages.\n        \"\"\"\n        return len(self.messages)\n\n    def __repr__(self):\n        \"\"\"Get string representation of memory state.\n\n        Returns:\n            str: String showing message count and token usage/limit.\n        \"\"\"\n        return f\"&lt;STM messages={len(self.messages)} tokens={self.total_tokens()}/{self.max_tokens}&gt;\"\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.__init__","title":"<code>__init__(max_tokens=2048)</code>","text":"<p>Initialize short-term memory with token limit.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to store. Defaults to 2048.</p> <code>2048</code> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def __init__(self, max_tokens: int = 2048):\n    \"\"\"Initialize short-term memory with token limit.\n\n    Args:\n        max_tokens (int, optional): Maximum number of tokens to store.\n            Defaults to 2048.\n    \"\"\"\n    self.messages: List[Message] = []\n    self.max_tokens = max_tokens\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.__len__","title":"<code>__len__()</code>","text":"<p>Get number of messages in memory.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Count of stored messages.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def __len__(self):\n    \"\"\"Get number of messages in memory.\n\n    Returns:\n        int: Count of stored messages.\n    \"\"\"\n    return len(self.messages)\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.__repr__","title":"<code>__repr__()</code>","text":"<p>Get string representation of memory state.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>String showing message count and token usage/limit.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def __repr__(self):\n    \"\"\"Get string representation of memory state.\n\n    Returns:\n        str: String showing message count and token usage/limit.\n    \"\"\"\n    return f\"&lt;STM messages={len(self.messages)} tokens={self.total_tokens()}/{self.max_tokens}&gt;\"\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.append","title":"<code>append(message)</code>","text":"<p>Add a message to memory and evict old messages if needed.</p> <p>If the message doesn't have an ID, generates a UUID for it. After adding the message, ensures token limit compliance by evicting old messages if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The message to add to memory.</p> required Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def append(self, message: Message) -&gt; None:\n    \"\"\"Add a message to memory and evict old messages if needed.\n\n    If the message doesn't have an ID, generates a UUID for it. After\n    adding the message, ensures token limit compliance by evicting old\n    messages if necessary.\n\n    Args:\n        message (Message): The message to add to memory.\n    \"\"\"\n    if not message.id:\n        message.id = str(uuid.uuid4())\n\n    self.messages.append(message)\n    self._evict_if_needed()\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.clear","title":"<code>clear()</code>","text":"<p>Remove all messages from memory.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Remove all messages from memory.\"\"\"\n    self.messages = []\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.get_messages","title":"<code>get_messages()</code>","text":"<p>Get all messages currently in memory.</p> <p>Returns:</p> Type Description <code>List[Message]</code> <p>List[Message]: List of all stored messages.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def get_messages(self) -&gt; List[Message]:\n    \"\"\"Get all messages currently in memory.\n\n    Returns:\n        List[Message]: List of all stored messages.\n    \"\"\"\n    return self.messages\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.set_messages","title":"<code>set_messages(messages)</code>","text":"<p>Replace current messages with new list and maintain token limit.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>New messages to store in memory.</p> required Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def set_messages(self, messages: List[Message]) -&gt; None:\n    \"\"\"Replace current messages with new list and maintain token limit.\n\n    Args:\n        messages (List[Message]): New messages to store in memory.\n    \"\"\"\n    self.messages = messages\n    self._evict_if_needed()\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.total_tokens","title":"<code>total_tokens()</code>","text":"<p>Calculate total tokens used by all messages.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Sum of estimated token lengths across all messages.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def total_tokens(self) -&gt; int:\n    \"\"\"Calculate total tokens used by all messages.\n\n    Returns:\n        int: Sum of estimated token lengths across all messages.\n    \"\"\"\n    return sum(m.estimated_token_length() for m in self.messages)\n</code></pre>"}]}