{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Neurotrace","text":"<p>A hybrid memory library designed for LangChain agents, providing dual-layer memory architecture with short-term buffer memory and long-term hybrid RAG system capabilities.</p>"},{"location":"#overview","title":"Overview","text":"<p>Neurotrace provides persistent, intelligent memory for conversational agents that improves over time and enables contextual understanding and recall. It combines vector-based and graph-based RAG (Retrieval Augmented Generation) systems to provide deeper and more accurate contextual reasoning.</p>"},{"location":"#key-features","title":"\ud83c\udfaf Key Features","text":"<ul> <li>Dual-Layer Memory Architecture</li> <li>Short-term buffer memory for immediate context</li> <li> <p>Long-term hybrid RAG system for persistent storage</p> </li> <li> <p>Real-time Processing</p> </li> <li>Real-time recall during conversations</li> <li> <p>Intelligent storage and compression</p> </li> <li> <p>Rich Message Structure</p> </li> <li>Custom metadata-rich message formats</li> <li> <p>Support for filtering and semantic tracing</p> </li> <li> <p>Hybrid Retrieval System</p> </li> <li>Combined vector and graph-based RAG</li> <li>Enhanced contextual reasoning capabilities</li> </ul>"},{"location":"#target-users","title":"\ud83c\udfaf Target Users","text":"<ul> <li>Developers building AI agents with LangChain</li> <li>Researchers exploring memory augmentation in LLMs</li> <li>Enterprises deploying context-aware AI assistants</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install neurotrace\n</code></pre>"},{"location":"#complete-example","title":"Complete Example","text":"<p>A complete, runnable example is available in <code>examples/agent_example.py</code>. This example demonstrates: - Setting up a Neurotrace agent with both short-term and long-term memory - Configuring vector and graph storage - Implementing an interactive conversation loop - Monitoring memory usage</p> <p>To run the example: <pre><code># First set up your environment variables\nexport NEO4J_URL=bolt://localhost:7687\nexport NEO4J_USERNAME=neo4j\nexport NEO4J_PASSWORD=your_password\nexport GOOGLE_API_KEY=your_google_api_key\n\n# Then run the example\npython examples/agent_example.py\n</code></pre></p>"},{"location":"#required-environment-variables","title":"Required Environment Variables","text":"<pre><code>NEO4J_URL=bolt://localhost:7687\nNEO4J_USERNAME=neo4j\nNEO4J_PASSWORD=your_password\nGOOGLE_API_KEY=your_google_api_key  # For Gemini LLM\n</code></pre>"},{"location":"#technical-documentation","title":"Technical Documentation","text":""},{"location":"#core-schema","title":"Core Schema","text":"<p>The <code>neurotrace.core.schema</code> module defines the fundamental data structures used throughout the project.</p>"},{"location":"#message","title":"Message","text":"<p>The core Message class represents a single message in the system:</p> <pre><code>from neurotrace.core.schema import Message, MessageMetadata, EmotionTag\n\nmessage = Message(\n    role=\"user\",           # Can be \"user\", \"ai\", or \"system\"\n    content=\"Hello!\",      # The message text content\n    metadata=MessageMetadata(\n        source=\"chat\",\n        emotions=EmotionTag(sentiment=\"positive\")\n    )\n)\n</code></pre> <p>Key features of Message: - Auto-generated UUID for each message - Automatic timestamp on creation - Type-safe role validation - Rich metadata support via MessageMetadata</p>"},{"location":"#message-components","title":"Message Components","text":""},{"location":"#emotiontag","title":"EmotionTag","text":"<p>Represents the emotional context of a message:</p> <pre><code>from neurotrace.core.schema import EmotionTag\n\nemotion = EmotionTag(\n    sentiment=\"positive\",  # Can be \"positive\", \"neutral\", or \"negative\"\n    intensity=0.8         # Optional float value indicating intensity\n)\n</code></pre>"},{"location":"#messagemetadata","title":"MessageMetadata","text":"<p>Contains additional information and context about a message:</p> <pre><code>from neurotrace.core.schema import MessageMetadata, EmotionTag\n\nmetadata = MessageMetadata(\n    token_count=150,                    # Number of tokens in the message\n    embedding=[0.1, 0.2, 0.3],         # Vector embedding for similarity search\n    source=\"chat\",                      # Source: \"chat\", \"web\", \"api\", or \"system\"\n    tags=[\"important\", \"follow-up\"],    # Custom tags\n    thread_id=\"thread_123\",            # Conversation thread identifier\n    user_id=\"user_456\",               # Associated user identifier\n    related_ids=[\"msg_789\"],          # Related message IDs\n    emotions=EmotionTag(sentiment=\"positive\"),  # Emotional context\n    compressed=False                   # Compression status\n)\n</code></pre> <p>Each field in MessageMetadata is optional and provides specific context: - <code>token_count</code>: Used for tracking token usage - <code>embedding</code>: Vector representation for similarity search - <code>source</code>: Indicates message origin - <code>tags</code>: Custom categorization - <code>thread_id</code>: Groups messages in conversations - <code>user_id</code>: Links messages to users - <code>related_ids</code>: Connects related messages - <code>emotions</code>: Captures emotional context - <code>compressed</code>: Indicates if content is compressed</p>"},{"location":"reference/documentation/core/constants/","title":"<code>neurotrace.core.constants</code>","text":""},{"location":"reference/documentation/core/constants/#neurotrace.core.constants","title":"<code>neurotrace.core.constants</code>","text":""},{"location":"reference/documentation/core/graph_memory/","title":"<code>neurotrace.core.graph_memory</code>","text":""},{"location":"reference/documentation/core/graph_memory/#neurotrace.core.graph_memory","title":"<code>neurotrace.core.graph_memory</code>","text":""},{"location":"reference/documentation/core/graph_memory/#neurotrace.core.graph_memory.GraphMemoryAdapter","title":"<code>GraphMemoryAdapter</code>","text":"<p>               Bases: <code>BaseGraphMemoryAdapter</code></p> Source code in <code>neurotrace/core/graph_memory.py</code> <pre><code>class GraphMemoryAdapter(BaseGraphMemoryAdapter):\n    def __init__(\n        self,\n        llm: Union[BaseLLM, BaseChatModel],\n        graph_database: GraphStore,\n        triplets_indexer: GraphTripletIndexerBase = None,\n    ):\n        self.llm = llm\n        self.graph = graph_database\n\n        self.triplets_indexer = triplets_indexer or GraphTripletIndexer(self.llm)\n        self.qa_chain = GraphCypherQAChain.from_llm(\n            llm=self.llm, graph=self.graph, verbose=True, allow_dangerous_requests=True  # Prints the generated Cypher\n        )\n\n    def insert_triplets(\n        self, triples, sender: Literal[\"user\", \"agent\"] = \"user\", timestamp: str = None, tags: List[str] = None\n    ):\n        if not triples:\n            print(\"No triplets to insert\")\n            return\n\n        if timestamp is None:\n            timestamp = datetime.now().isoformat()\n\n        if len(triples) &gt; 3:\n            print(\"Too many triplets\", triples)\n            return\n\n        s, r, o = triples\n        query = f\"\"\"\n        MERGE (a:Entity {{name: $s}})\n          ON CREATE SET a.created_at = $timestamp\n\n        MERGE (b:Entity {{name: $o}})\n          ON CREATE SET b.created_at = $timestamp\n\n        MERGE (a)-[r:`{r.upper().replace(\" \", \"_\")}`]-&gt;(b)\n          ON CREATE SET r.created_at = $timestamp, r.source = $sender\n          ON CREATE SET r.tags = $tags\n        \"\"\"\n        self.graph.query(\n            query, params={\"s\": s, \"o\": o, \"r\": r, \"timestamp\": timestamp, \"sender\": sender, \"tags\": tags or []}\n        )\n\n    def add_conversation(\n        self, summarised_text: str, sender: Literal[\"user\", \"agent\"] = \"agent\", tags: List[str] = None\n    ):\n        \"\"\"\n        Add a conversation to the graph memory.\n\n        Args:\n            sender (Literal[\"user\", \"agent\"]): The sender of the message.\n            :param summarised_text:\n        \"\"\"\n        triplets = self.triplets_indexer.extract(summarised_text)\n\n        for triplet in triplets:\n            self.insert_triplets(triplet, sender=sender, tags=tags)\n\n    def ask_graph(self, query: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Ask a question to the graph memory and get an answer.\n\n        Args:\n            query (str): The question to ask.\n\n        Returns:\n            str: The answer from the graph memory.\n        \"\"\"\n        return self.qa_chain.invoke({\"query\": query})\n\n    def get_all_relation_types(self) -&gt; list[str]:\n        result = self.graph.query(\"CALL db.relationshipTypes()\")\n        # Output is like: [{'relationshipType': 'WORKS_AT'}, ...]\n        return [record[\"relationshipType\"] for record in result]\n</code></pre>"},{"location":"reference/documentation/core/graph_memory/#neurotrace.core.graph_memory.GraphMemoryAdapter.add_conversation","title":"<code>add_conversation(summarised_text, sender='agent', tags=None)</code>","text":"<p>Add a conversation to the graph memory.</p> <p>Parameters:</p> Name Type Description Default <code>sender</code> <code>Literal['user', 'agent']</code> <p>The sender of the message.</p> <code>'agent'</code> <code></code> <p>param summarised_text:</p> required Source code in <code>neurotrace/core/graph_memory.py</code> <pre><code>def add_conversation(\n    self, summarised_text: str, sender: Literal[\"user\", \"agent\"] = \"agent\", tags: List[str] = None\n):\n    \"\"\"\n    Add a conversation to the graph memory.\n\n    Args:\n        sender (Literal[\"user\", \"agent\"]): The sender of the message.\n        :param summarised_text:\n    \"\"\"\n    triplets = self.triplets_indexer.extract(summarised_text)\n\n    for triplet in triplets:\n        self.insert_triplets(triplet, sender=sender, tags=tags)\n</code></pre>"},{"location":"reference/documentation/core/graph_memory/#neurotrace.core.graph_memory.GraphMemoryAdapter.ask_graph","title":"<code>ask_graph(query)</code>","text":"<p>Ask a question to the graph memory and get an answer.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The question to ask.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Dict[str, Any]</code> <p>The answer from the graph memory.</p> Source code in <code>neurotrace/core/graph_memory.py</code> <pre><code>def ask_graph(self, query: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Ask a question to the graph memory and get an answer.\n\n    Args:\n        query (str): The question to ask.\n\n    Returns:\n        str: The answer from the graph memory.\n    \"\"\"\n    return self.qa_chain.invoke({\"query\": query})\n</code></pre>"},{"location":"reference/documentation/core/llm_tasks/","title":"<code>neurotrace.core.llm_tasks</code>","text":""},{"location":"reference/documentation/core/llm_tasks/#neurotrace.core.llm_tasks","title":"<code>neurotrace.core.llm_tasks</code>","text":""},{"location":"reference/documentation/core/llm_tasks/#neurotrace.core.llm_tasks.get_graph_summary","title":"<code>get_graph_summary(llm, text)</code>","text":"<p>Get a graph summary from the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseLLM</code> <p>The language model to use for summarisation.</p> required <code>text</code> <code>str</code> <p>The input text to summarize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The graph summary generated by the LLM.</p> Source code in <code>neurotrace/core/llm_tasks.py</code> <pre><code>def get_graph_summary(llm: BaseLLM, text: str) -&gt; str:\n    \"\"\"\n    Get a graph summary from the LLM.\n\n    Args:\n        llm (BaseLLM): The language model to use for summarisation.\n        text (str): The input text to summarize.\n\n    Returns:\n        str: The graph summary generated by the LLM.\n    \"\"\"\n    response = _perform_summarisation(llm=llm, prompt=task_prompts.PROMPT_GRAPH_SUMMARY, message=text)\n    response = strip_json_code_block(response)\n    return response\n</code></pre>"},{"location":"reference/documentation/core/llm_tasks/#neurotrace.core.llm_tasks.get_vector_and_graph_summary","title":"<code>get_vector_and_graph_summary(llm, text)</code>","text":"<p>Get vector and graph summaries from the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseLLM</code> <p>The language model to use for summarisation.</p> required <code>text</code> <code>str</code> <p>The input text to summarize.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Tuple[str, str]: A tuple containing the vector summary and graph summary.</p> Source code in <code>neurotrace/core/llm_tasks.py</code> <pre><code>def get_vector_and_graph_summary(llm: BaseLLM, text: str) -&gt; Dict[str, str]:\n    \"\"\"\n    Get vector and graph summaries from the LLM.\n\n    Args:\n        llm (BaseLLM): The language model to use for summarisation.\n        text (str): The input text to summarize.\n\n    Returns:\n        Tuple[str, str]: A tuple containing the vector summary and graph summary.\n    \"\"\"\n    response = _perform_summarisation(llm=llm, prompt=task_prompts.PROMPT_VECTOR_AND_GRAPH_SUMMARY, message=text)\n    response = strip_json_code_block(response)\n    try:\n        return json.loads(response)\n    except json.decoder.JSONDecodeError:\n        return {}\n</code></pre>"},{"location":"reference/documentation/core/llm_tasks/#neurotrace.core.llm_tasks.perform_summarisation","title":"<code>perform_summarisation(llm, prompt_placeholders, prompt=None)</code>","text":"<p>Perform summarisation using the provided LLM and prompt with a single message.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseLLM</code> <p>The language model to use for summarisation.</p> required <code>prompt</code> <code>PromptTemplate</code> <p>The prompt with any number of variables.</p> <code>None</code> <code>message</code> <code>str</code> <p>The input text to summarise.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The summarized or generated output from the LLM.</p> <code>str</code> <p>param prompt_placeholders:</p> Source code in <code>neurotrace/core/llm_tasks.py</code> <pre><code>def perform_summarisation(llm: BaseLLM, prompt_placeholders: Dict[str, Any], prompt: PromptTemplate = None) -&gt; str:\n    \"\"\"\n    Perform summarisation using the provided LLM and prompt with a single message.\n\n    Args:\n        llm (BaseLLM): The language model to use for summarisation.\n        prompt (PromptTemplate): The prompt with any number of variables.\n        message (str): The input text to summarise.\n\n    Returns:\n        str: The summarized or generated output from the LLM.\n        :param prompt_placeholders:\n    \"\"\"\n    prompt = prompt or task_prompts.PROMPT_GENERAL_SUMMARY\n    return _perform_summarisation(llm=llm, prompt=prompt, **prompt_placeholders)\n</code></pre>"},{"location":"reference/documentation/core/memory/","title":"<code>neurotrace.core.memory</code>","text":""},{"location":"reference/documentation/core/memory/#neurotrace.core.memory","title":"<code>neurotrace.core.memory</code>","text":""},{"location":"reference/documentation/core/memory/#neurotrace.core.memory.NeurotraceMemory","title":"<code>NeurotraceMemory</code>","text":"<p>               Bases: <code>BaseMemory</code></p> <p>A LangChain-compatible memory implementation using a hybrid memory system.</p> <p>This class implements a memory system that combines short-term memory (STM) and optional long-term memory (LTM) capabilities. It wraps the ShortTermMemory component and integrates with LangChain's memory interface.</p> <p>Attributes:</p> Name Type Description <code>session_id</code> <code>str</code> <p>Unique identifier for the current chat session.</p> <code>_stm</code> <code>ShortTermMemory</code> <p>Short-term memory component with token limit.</p> <code>_ltm</code> <code>LongTermMemory</code> <p>Optional long-term memory adapter for persistence.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to store in short-term memory. Defaults to 2048.</p> <code>2048</code> <code>history</code> <code>BaseChatMessageHistory</code> <p>LangChain chat history for long-term storage. If provided, enables long-term memory. Defaults to None.</p> <code>None</code> <code>session_id</code> <code>str</code> <p>Identifier for the chat session. Defaults to \"default\".</p> <code>'default'</code> Source code in <code>neurotrace/core/memory.py</code> <pre><code>class NeurotraceMemory(BaseMemory):\n    \"\"\"A LangChain-compatible memory implementation using a hybrid memory system.\n\n    This class implements a memory system that combines short-term memory (STM) and\n    optional long-term memory (LTM) capabilities. It wraps the ShortTermMemory\n    component and integrates with LangChain's memory interface.\n\n    Attributes:\n        session_id (str): Unique identifier for the current chat session.\n        _stm (ShortTermMemory): Short-term memory component with token limit.\n        _ltm (LongTermMemory): Optional long-term memory adapter for persistence.\n\n    Args:\n        max_tokens (int, optional): Maximum number of tokens to store in short-term memory.\n            Defaults to 2048.\n        history (BaseChatMessageHistory, optional): LangChain chat history for long-term\n            storage. If provided, enables long-term memory. Defaults to None.\n        session_id (str, optional): Identifier for the chat session. Defaults to \"default\".\n    \"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra=\"allow\",\n    )\n\n    def __init__(\n        self,\n        llm: Union[BaseLLM, BaseChatModel],\n        session_id: str = \"default\",\n        max_tokens: int = 2048,\n        history: BaseChatMessageHistory = None,\n    ):\n        super().__init__()\n        self.llm = llm\n        self.session_id = session_id\n        self._stm = ShortTermMemory(max_tokens=max_tokens)\n        self._ltm = LongTermMemory(history, session_id=session_id) if history else None\n\n    @property\n    def memory_variables(self) -&gt; List[str]:\n        \"\"\"Gets the list of memory variables used by this memory component.\n\n        Returns:\n            List[str]: List containing \"chat_history\" as the only memory variable.\n        \"\"\"\n        return [\"chat_history\"]\n\n    def load_memory_variables(self, inputs: Dict[str, Any]) -&gt; Dict[str, List[BaseMessage]]:\n        \"\"\"Retrieves the current memory state as LangChain messages.\n\n        Converts all messages in short-term memory to LangChain's message format\n        for compatibility with the LangChain framework.\n\n        Args:\n            inputs (Dict[str, Any]): Input variables (unused in this implementation).\n\n        Returns:\n            Dict[str, List[BaseMessage]]: Dictionary with \"chat_history\" key containing\n                the list of messages in LangChain format.\n        \"\"\"\n        return {\"chat_history\": [msg.to_langchain_message() for msg in self._stm.get_messages()]}\n\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, Any]) -&gt; None:\n        \"\"\"Saves the conversation context to both short-term and long-term memory.\n\n        Creates Message objects from the input and output and stores them in\n        short-term memory. If long-term memory is enabled, also saves to it.\n\n        Args:\n            inputs (Dict[str, Any]): Dictionary containing user input with key \"input\".\n            outputs (Dict[str, Any]): Dictionary containing AI output with key \"output\".\n        \"\"\"\n        user_input = inputs.get(\"input\") or \"\"\n        ai_output = outputs.get(\"output\") or \"\"\n\n        # Build Message objects\n        user_msg = Message(\n            role=str(Role.HUMAN),\n            content=user_input,\n            metadata=MessageMetadata(\n                session_id=self.session_id,\n            ),\n        )\n        ai_msg = Message(\n            role=str(Role.AI),\n            content=ai_output,\n            metadata=MessageMetadata(\n                session_id=self.session_id,\n            ),\n        )\n\n        # Save in short-term memory\n        self._stm.append(user_msg)\n        self._stm.append(ai_msg)\n\n        if self._ltm:\n            self._ltm.add_message(user_msg)\n            self._ltm.add_message(ai_msg)\n\n    def clear(self, delete_history: bool = False) -&gt; None:\n        \"\"\"Clears the memory state.\n\n        Clears the short-term memory and optionally the long-term memory if specified.\n\n        Args:\n            delete_history (bool, optional): If True, also clears long-term memory\n                if it exists. Defaults to False.\n        \"\"\"\n        self._stm.clear()\n        if self._ltm and delete_history:\n            self._ltm.clear()\n</code></pre>"},{"location":"reference/documentation/core/memory/#neurotrace.core.memory.NeurotraceMemory.memory_variables","title":"<code>memory_variables</code>  <code>property</code>","text":"<p>Gets the list of memory variables used by this memory component.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List containing \"chat_history\" as the only memory variable.</p>"},{"location":"reference/documentation/core/memory/#neurotrace.core.memory.NeurotraceMemory.clear","title":"<code>clear(delete_history=False)</code>","text":"<p>Clears the memory state.</p> <p>Clears the short-term memory and optionally the long-term memory if specified.</p> <p>Parameters:</p> Name Type Description Default <code>delete_history</code> <code>bool</code> <p>If True, also clears long-term memory if it exists. Defaults to False.</p> <code>False</code> Source code in <code>neurotrace/core/memory.py</code> <pre><code>def clear(self, delete_history: bool = False) -&gt; None:\n    \"\"\"Clears the memory state.\n\n    Clears the short-term memory and optionally the long-term memory if specified.\n\n    Args:\n        delete_history (bool, optional): If True, also clears long-term memory\n            if it exists. Defaults to False.\n    \"\"\"\n    self._stm.clear()\n    if self._ltm and delete_history:\n        self._ltm.clear()\n</code></pre>"},{"location":"reference/documentation/core/memory/#neurotrace.core.memory.NeurotraceMemory.load_memory_variables","title":"<code>load_memory_variables(inputs)</code>","text":"<p>Retrieves the current memory state as LangChain messages.</p> <p>Converts all messages in short-term memory to LangChain's message format for compatibility with the LangChain framework.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Any]</code> <p>Input variables (unused in this implementation).</p> required <p>Returns:</p> Type Description <code>Dict[str, List[BaseMessage]]</code> <p>Dict[str, List[BaseMessage]]: Dictionary with \"chat_history\" key containing the list of messages in LangChain format.</p> Source code in <code>neurotrace/core/memory.py</code> <pre><code>def load_memory_variables(self, inputs: Dict[str, Any]) -&gt; Dict[str, List[BaseMessage]]:\n    \"\"\"Retrieves the current memory state as LangChain messages.\n\n    Converts all messages in short-term memory to LangChain's message format\n    for compatibility with the LangChain framework.\n\n    Args:\n        inputs (Dict[str, Any]): Input variables (unused in this implementation).\n\n    Returns:\n        Dict[str, List[BaseMessage]]: Dictionary with \"chat_history\" key containing\n            the list of messages in LangChain format.\n    \"\"\"\n    return {\"chat_history\": [msg.to_langchain_message() for msg in self._stm.get_messages()]}\n</code></pre>"},{"location":"reference/documentation/core/memory/#neurotrace.core.memory.NeurotraceMemory.save_context","title":"<code>save_context(inputs, outputs)</code>","text":"<p>Saves the conversation context to both short-term and long-term memory.</p> <p>Creates Message objects from the input and output and stores them in short-term memory. If long-term memory is enabled, also saves to it.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Any]</code> <p>Dictionary containing user input with key \"input\".</p> required <code>outputs</code> <code>Dict[str, Any]</code> <p>Dictionary containing AI output with key \"output\".</p> required Source code in <code>neurotrace/core/memory.py</code> <pre><code>def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, Any]) -&gt; None:\n    \"\"\"Saves the conversation context to both short-term and long-term memory.\n\n    Creates Message objects from the input and output and stores them in\n    short-term memory. If long-term memory is enabled, also saves to it.\n\n    Args:\n        inputs (Dict[str, Any]): Dictionary containing user input with key \"input\".\n        outputs (Dict[str, Any]): Dictionary containing AI output with key \"output\".\n    \"\"\"\n    user_input = inputs.get(\"input\") or \"\"\n    ai_output = outputs.get(\"output\") or \"\"\n\n    # Build Message objects\n    user_msg = Message(\n        role=str(Role.HUMAN),\n        content=user_input,\n        metadata=MessageMetadata(\n            session_id=self.session_id,\n        ),\n    )\n    ai_msg = Message(\n        role=str(Role.AI),\n        content=ai_output,\n        metadata=MessageMetadata(\n            session_id=self.session_id,\n        ),\n    )\n\n    # Save in short-term memory\n    self._stm.append(user_msg)\n    self._stm.append(ai_msg)\n\n    if self._ltm:\n        self._ltm.add_message(user_msg)\n        self._ltm.add_message(ai_msg)\n</code></pre>"},{"location":"reference/documentation/core/schema/","title":"<code>neurotrace.core.schema</code>","text":""},{"location":"reference/documentation/core/schema/#neurotrace.core.schema","title":"<code>neurotrace.core.schema</code>","text":"<p>Schema definitions for neurotrace core components.</p> <p>This module defines the data structures used for managing neuromorphic memory components including messages, metadata, and emotion tags using Pydantic models.</p> Note <p>All models inherit from Pydantic BaseModel for data validation.</p>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.EmotionTag","title":"<code>EmotionTag</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the emotional context and intensity of a message.</p> <p>Attributes:</p> Name Type Description <code>sentiment</code> <code>Optional[Literal['positive', 'neutral', 'negative']]</code> <p>The emotional tone of the message. Defaults to None.</p> <code>intensity</code> <code>Optional[float]</code> <p>A value indicating the strength of the emotion. Defaults to None.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>class EmotionTag(BaseModel):\n    \"\"\"Represents the emotional context and intensity of a message.\n\n    Attributes:\n        sentiment (Optional[Literal[\"positive\", \"neutral\", \"negative\"]]): The emotional\n            tone of the message. Defaults to None.\n        intensity (Optional[float]): A value indicating the strength of the emotion.\n            Defaults to None.\n    \"\"\"\n\n    sentiment: Optional[Literal[\"positive\", \"neutral\", \"negative\"]] = None\n    intensity: Optional[float] = None\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message","title":"<code>Message</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Message represents a single communication in the system. It includes the sender's role, content, timestamp, and metadata. Each message has a unique identifier generated as a UUID.</p> <p>Example Representation: {     \"id\": \"\",                    # unique message ID     \"role\": \"user\" | \"ai\" | \"system\",           # sender role     \"content\": \"string\",                        # message text     \"timestamp\": \"ISO 8601\",                    # message time (UTC)     \"metadata\": {         \"token_count\": 32,                      # optional, for budgeting/compression         \"embedding\": [...],                     # vector representation (optional in-memory or precomputed)         \"source\": \"chat\" | \"web\" | \"api\",       # source of message         \"tags\": [\"finance\", \"personal\"],        # custom tags for search         \"thread_id\": \"conversation_XYZ\",        # optional thread/conversation tracking         \"user_id\": \"abc123\",                    # to associate memory across sessions         \"related_ids\": [\"msg_id_1\", \"msg_id_2\"],# links to other related messages (graph edge)         \"emotions\": {\"sentiment\": \"positive\", \"intensity\": 0.85},  # optional emotion tagging         \"compressed\": False                     # for summarization/compression tracking         \"session_id\": \"default\"                # session identifier for context     } } Source code in <code>neurotrace/core/schema.py</code> <pre><code>class Message(BaseModel):\n    \"\"\"\n    Message represents a single communication in the system.\n    It includes the sender's role, content, timestamp, and metadata.\n    Each message has a unique identifier generated as a UUID.\n\n    Example Representation:\n    {\n        \"id\": \"&lt;uuid4 or hash&gt;\",                    # unique message ID\n        \"role\": \"user\" | \"ai\" | \"system\",           # sender role\n        \"content\": \"string\",                        # message text\n        \"timestamp\": \"ISO 8601\",                    # message time (UTC)\n        \"metadata\": {\n            \"token_count\": 32,                      # optional, for budgeting/compression\n            \"embedding\": [...],                     # vector representation (optional in-memory or precomputed)\n            \"source\": \"chat\" | \"web\" | \"api\",       # source of message\n            \"tags\": [\"finance\", \"personal\"],        # custom tags for search\n            \"thread_id\": \"conversation_XYZ\",        # optional thread/conversation tracking\n            \"user_id\": \"abc123\",                    # to associate memory across sessions\n            \"related_ids\": [\"msg_id_1\", \"msg_id_2\"],# links to other related messages (graph edge)\n            \"emotions\": {\"sentiment\": \"positive\", \"intensity\": 0.85},  # optional emotion tagging\n            \"compressed\": False                     # for summarization/compression tracking\n            \"session_id\": \"default\"                # session identifier for context\n        }\n    }\n    \"\"\"\n\n    id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    role: str\n    content: str\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(UTC))\n    metadata: MessageMetadata = Field(default_factory=MessageMetadata)\n\n    def estimated_token_length(self) -&gt; int:\n        \"\"\"Estimates the number of tokens in the message content.\n\n        Returns:\n            int: The estimated token count, either from metadata or word-based count.\n\n        Note:\n            Currently uses a simple word-splitting approach if token_count is not set\n            in metadata. TODO: Implement a more accurate token counting method.\n        \"\"\"\n        # todo: Implement a more accurate token counting method\n        return self.metadata.token_count or len(self.content.split())\n\n    def to_langchain_message(self) -&gt; Union[HumanMessage, AIMessage]:\n        \"\"\"Converts this Message to a LangChain compatible format.\n\n        Returns:\n            Union[HumanMessage, AIMessage]: A LangChain message object based on the role.\n\n        Raises:\n            ValueError: If the role is neither 'human' nor 'ai'.\n        \"\"\"\n        if Role.from_string(self.role) is Role.HUMAN:\n            return self.to_human_message()\n        elif Role.from_string(self.role) is Role.AI:\n            return self.to_ai_message()\n        else:\n            raise ValueError(f\"Unsupported role: {self.role}. Use 'human' or 'ai'.\")\n\n    def to_human_message(self) -&gt; HumanMessage:\n        \"\"\"Converts this Message to a LangChain HumanMessage format.\n\n        Returns:\n            HumanMessage: A LangChain HumanMessage with the message content and metadata.\n        \"\"\"\n        return HumanMessage(\n            id=self.id, content=self.content, additional_kwargs={\"id\": self.id, \"metadata\": self.metadata.model_dump()}\n        )\n\n    def to_ai_message(self) -&gt; AIMessage:\n        \"\"\"Converts this Message to a LangChain AIMessage format.\n\n        Returns:\n            AIMessage: A LangChain AIMessage with the message content and metadata.\n        \"\"\"\n        return AIMessage(\n            id=self.id, content=self.content, additional_kwargs={\"id\": self.id, \"metadata\": self.metadata.model_dump()}\n        )\n\n    def to_document(self) -&gt; Document:\n        \"\"\"Convert Message to LangChain-compatible Document with safe metadata.\n\n        Converts the current Message instance to a LangChain Document format,\n        ensuring that the metadata is properly serialized and complex types\n        are filtered out.\n\n        Returns:\n            Document: A LangChain Document instance containing the message content\n                and filtered metadata.\n        \"\"\"\n        raw_metadata = self.metadata.model_dump() if self.metadata else {}\n        doc = Document(page_content=self.content, metadata={\"id\": self.id, \"role\": self.role, **raw_metadata})\n        doc = filter_complex_metadata([doc])  # Remove lists, dicts, etc.\n\n        return doc[0]\n\n    @staticmethod\n    def from_document(doc: Document) -&gt; \"Message\":\n        \"\"\"Creates a Message instance from a LangChain Document.\n\n        Extracts content and metadata from a LangChain Document to create\n        a new Message instance. The role is extracted from metadata with\n        a fallback to HUMAN if not specified.\n\n        Args:\n            doc (Document): The LangChain Document to convert from.\n\n        Returns:\n            Message: A new Message instance containing the document's content\n                and metadata.\n        \"\"\"\n        metadata = doc.metadata or {}\n        role_str = metadata.pop(\"role\", Role.HUMAN.value)  # fallback to human\n        return Message(\n            role=Role.from_string(role_str),\n            content=doc.page_content,\n            metadata=MessageMetadata(**metadata) if metadata else None,\n            id=metadata.get(\"id\"),\n        )\n\n    def __eq__(self, other):\n        \"\"\"Checks equality between two Message objects.\n\n        Compares two Message instances for equality based on their role,\n        content, and metadata. The id field is intentionally excluded from\n        the comparison.\n\n        Args:\n            other: Another object to compare with.\n\n        Returns:\n            bool: True if the messages have the same role, content, and metadata\n                (excluding id), False otherwise.\n        \"\"\"\n        if not isinstance(other, Message):\n            return False\n\n        # not comparing id\n        return self.role == other.role and self.content == other.content and self.metadata == other.metadata\n\n    def __repr__(self):\n        \"\"\"String representation of the Message object.\n\n        Returns:\n            str: A string representation of the Message, including role, content,\n                and metadata.\n        \"\"\"\n        return f\"[{self.timestamp.date()}] ({self.role}): {self.content}\"\n\n    def __str__(self):\n        \"\"\"String representation of the Message object.\n\n        Returns:\n            str: A string representation of the Message, including role, content,\n                and metadata.\n        \"\"\"\n        return f\"[{self.timestamp.isoformat()}] ({self.role}): {self.content}\"\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Checks equality between two Message objects.</p> <p>Compares two Message instances for equality based on their role, content, and metadata. The id field is intentionally excluded from the comparison.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <p>Another object to compare with.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the messages have the same role, content, and metadata (excluding id), False otherwise.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def __eq__(self, other):\n    \"\"\"Checks equality between two Message objects.\n\n    Compares two Message instances for equality based on their role,\n    content, and metadata. The id field is intentionally excluded from\n    the comparison.\n\n    Args:\n        other: Another object to compare with.\n\n    Returns:\n        bool: True if the messages have the same role, content, and metadata\n            (excluding id), False otherwise.\n    \"\"\"\n    if not isinstance(other, Message):\n        return False\n\n    # not comparing id\n    return self.role == other.role and self.content == other.content and self.metadata == other.metadata\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of the Message object.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A string representation of the Message, including role, content, and metadata.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def __repr__(self):\n    \"\"\"String representation of the Message object.\n\n    Returns:\n        str: A string representation of the Message, including role, content,\n            and metadata.\n    \"\"\"\n    return f\"[{self.timestamp.date()}] ({self.role}): {self.content}\"\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.__str__","title":"<code>__str__()</code>","text":"<p>String representation of the Message object.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A string representation of the Message, including role, content, and metadata.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def __str__(self):\n    \"\"\"String representation of the Message object.\n\n    Returns:\n        str: A string representation of the Message, including role, content,\n            and metadata.\n    \"\"\"\n    return f\"[{self.timestamp.isoformat()}] ({self.role}): {self.content}\"\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.estimated_token_length","title":"<code>estimated_token_length()</code>","text":"<p>Estimates the number of tokens in the message content.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The estimated token count, either from metadata or word-based count.</p> Note <p>Currently uses a simple word-splitting approach if token_count is not set in metadata. TODO: Implement a more accurate token counting method.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def estimated_token_length(self) -&gt; int:\n    \"\"\"Estimates the number of tokens in the message content.\n\n    Returns:\n        int: The estimated token count, either from metadata or word-based count.\n\n    Note:\n        Currently uses a simple word-splitting approach if token_count is not set\n        in metadata. TODO: Implement a more accurate token counting method.\n    \"\"\"\n    # todo: Implement a more accurate token counting method\n    return self.metadata.token_count or len(self.content.split())\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.from_document","title":"<code>from_document(doc)</code>  <code>staticmethod</code>","text":"<p>Creates a Message instance from a LangChain Document.</p> <p>Extracts content and metadata from a LangChain Document to create a new Message instance. The role is extracted from metadata with a fallback to HUMAN if not specified.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Document</code> <p>The LangChain Document to convert from.</p> required <p>Returns:</p> Name Type Description <code>Message</code> <code>Message</code> <p>A new Message instance containing the document's content and metadata.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>@staticmethod\ndef from_document(doc: Document) -&gt; \"Message\":\n    \"\"\"Creates a Message instance from a LangChain Document.\n\n    Extracts content and metadata from a LangChain Document to create\n    a new Message instance. The role is extracted from metadata with\n    a fallback to HUMAN if not specified.\n\n    Args:\n        doc (Document): The LangChain Document to convert from.\n\n    Returns:\n        Message: A new Message instance containing the document's content\n            and metadata.\n    \"\"\"\n    metadata = doc.metadata or {}\n    role_str = metadata.pop(\"role\", Role.HUMAN.value)  # fallback to human\n    return Message(\n        role=Role.from_string(role_str),\n        content=doc.page_content,\n        metadata=MessageMetadata(**metadata) if metadata else None,\n        id=metadata.get(\"id\"),\n    )\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.to_ai_message","title":"<code>to_ai_message()</code>","text":"<p>Converts this Message to a LangChain AIMessage format.</p> <p>Returns:</p> Name Type Description <code>AIMessage</code> <code>AIMessage</code> <p>A LangChain AIMessage with the message content and metadata.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def to_ai_message(self) -&gt; AIMessage:\n    \"\"\"Converts this Message to a LangChain AIMessage format.\n\n    Returns:\n        AIMessage: A LangChain AIMessage with the message content and metadata.\n    \"\"\"\n    return AIMessage(\n        id=self.id, content=self.content, additional_kwargs={\"id\": self.id, \"metadata\": self.metadata.model_dump()}\n    )\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.to_document","title":"<code>to_document()</code>","text":"<p>Convert Message to LangChain-compatible Document with safe metadata.</p> <p>Converts the current Message instance to a LangChain Document format, ensuring that the metadata is properly serialized and complex types are filtered out.</p> <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>A LangChain Document instance containing the message content and filtered metadata.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def to_document(self) -&gt; Document:\n    \"\"\"Convert Message to LangChain-compatible Document with safe metadata.\n\n    Converts the current Message instance to a LangChain Document format,\n    ensuring that the metadata is properly serialized and complex types\n    are filtered out.\n\n    Returns:\n        Document: A LangChain Document instance containing the message content\n            and filtered metadata.\n    \"\"\"\n    raw_metadata = self.metadata.model_dump() if self.metadata else {}\n    doc = Document(page_content=self.content, metadata={\"id\": self.id, \"role\": self.role, **raw_metadata})\n    doc = filter_complex_metadata([doc])  # Remove lists, dicts, etc.\n\n    return doc[0]\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.to_human_message","title":"<code>to_human_message()</code>","text":"<p>Converts this Message to a LangChain HumanMessage format.</p> <p>Returns:</p> Name Type Description <code>HumanMessage</code> <code>HumanMessage</code> <p>A LangChain HumanMessage with the message content and metadata.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def to_human_message(self) -&gt; HumanMessage:\n    \"\"\"Converts this Message to a LangChain HumanMessage format.\n\n    Returns:\n        HumanMessage: A LangChain HumanMessage with the message content and metadata.\n    \"\"\"\n    return HumanMessage(\n        id=self.id, content=self.content, additional_kwargs={\"id\": self.id, \"metadata\": self.metadata.model_dump()}\n    )\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.Message.to_langchain_message","title":"<code>to_langchain_message()</code>","text":"<p>Converts this Message to a LangChain compatible format.</p> <p>Returns:</p> Type Description <code>Union[HumanMessage, AIMessage]</code> <p>Union[HumanMessage, AIMessage]: A LangChain message object based on the role.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the role is neither 'human' nor 'ai'.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>def to_langchain_message(self) -&gt; Union[HumanMessage, AIMessage]:\n    \"\"\"Converts this Message to a LangChain compatible format.\n\n    Returns:\n        Union[HumanMessage, AIMessage]: A LangChain message object based on the role.\n\n    Raises:\n        ValueError: If the role is neither 'human' nor 'ai'.\n    \"\"\"\n    if Role.from_string(self.role) is Role.HUMAN:\n        return self.to_human_message()\n    elif Role.from_string(self.role) is Role.AI:\n        return self.to_ai_message()\n    else:\n        raise ValueError(f\"Unsupported role: {self.role}. Use 'human' or 'ai'.\")\n</code></pre>"},{"location":"reference/documentation/core/schema/#neurotrace.core.schema.MessageMetadata","title":"<code>MessageMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Contains additional contextual information about a message.</p> <p>Attributes:</p> Name Type Description <code>token_count</code> <code>Optional[int]</code> <p>Number of tokens in the associated message.</p> <code>embedding</code> <code>Optional[List[float]]</code> <p>Vector representation of the message content.</p> <code>source</code> <code>Optional[Literal['chat', 'web', 'api', 'system']]</code> <p>Origin of the message. Defaults to \"chat\".</p> <code>tags</code> <code>Optional[List[str]]</code> <p>List of categorical tags. Defaults to empty list.</p> <code>thread_id</code> <code>Optional[str]</code> <p>Unique identifier for the conversation thread.</p> <code>user_id</code> <code>Optional[str]</code> <p>Identifier for the message author.</p> <code>related_ids</code> <code>Optional[List[str]]</code> <p>References to related message IDs.</p> <code>emotions</code> <code>Optional[EmotionTag]</code> <p>Emotional analysis of the message.</p> <code>compressed</code> <code>Optional[bool]</code> <p>Whether the message content is compressed. Defaults to False.</p> <code>session_id</code> <code>Optional[str]</code> <p>Current session identifier. Defaults to 'default'.</p> Source code in <code>neurotrace/core/schema.py</code> <pre><code>class MessageMetadata(BaseModel):\n    \"\"\"Contains additional contextual information about a message.\n\n    Attributes:\n        token_count (Optional[int]): Number of tokens in the associated message.\n        embedding (Optional[List[float]]): Vector representation of the message content.\n        source (Optional[Literal[\"chat\", \"web\", \"api\", \"system\"]]): Origin of the message.\n            Defaults to \"chat\".\n        tags (Optional[List[str]]): List of categorical tags. Defaults to empty list.\n        thread_id (Optional[str]): Unique identifier for the conversation thread.\n        user_id (Optional[str]): Identifier for the message author.\n        related_ids (Optional[List[str]]): References to related message IDs.\n        emotions (Optional[EmotionTag]): Emotional analysis of the message.\n        compressed (Optional[bool]): Whether the message content is compressed.\n            Defaults to False.\n        session_id (Optional[str]): Current session identifier. Defaults to 'default'.\n    \"\"\"\n\n    token_count: Optional[int] = None\n    embedding: Optional[List[float]] = None\n    source: Optional[Literal[\"chat\", \"web\", \"api\", \"system\"]] = \"chat\"\n    tags: Optional[List[str]] = []\n    thread_id: Optional[str] = None\n    user_id: Optional[str] = None\n    related_ids: Optional[List[str]] = []\n    emotions: Optional[EmotionTag] = None\n    compressed: Optional[bool] = False\n    session_id: Optional[str] = \"default\"\n</code></pre>"},{"location":"reference/documentation/core/utils/","title":"<code>neurotrace.core.utils</code>","text":""},{"location":"reference/documentation/core/utils/#neurotrace.core.utils","title":"<code>neurotrace.core.utils</code>","text":""},{"location":"reference/documentation/core/utils/#neurotrace.core.utils.load_prompt","title":"<code>load_prompt(name)</code>","text":"<p>Load a prompt file from the prompts directory.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the prompt file (without .md)</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Prompt text</p> Source code in <code>neurotrace/core/utils.py</code> <pre><code>def load_prompt(name: str) -&gt; str:\n    \"\"\"\n    Load a prompt file from the prompts directory.\n\n    Args:\n        name (str): Name of the prompt file (without .md)\n\n    Returns:\n        str: Prompt text\n    \"\"\"\n    path = os.path.join(PROMPT_DIR, \"tools\", f\"{name}.md\")\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Prompt file {path} does not exist.\")\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        return f.read()\n</code></pre>"},{"location":"reference/documentation/core/utils/#neurotrace.core.utils.safe_json_loads","title":"<code>safe_json_loads(json_string, return_type=dict)</code>","text":"<p>Safely load a JSON string, returning an empty dictionary on failure.</p> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>str</code> <p>The JSON string to load.</p> required <p>Returns:</p> Type Description <code>type</code> <p>param json_string:</p> <code>type</code> <p>param return_type:</p> Source code in <code>neurotrace/core/utils.py</code> <pre><code>def safe_json_loads(json_string: str, return_type: type = dict) -&gt; type:\n    \"\"\"\n    Safely load a JSON string, returning an empty dictionary on failure.\n\n    Args:\n        json_string (str): The JSON string to load.\n\n    Returns:\n        :param json_string:\n        :param return_type:\n    \"\"\"\n    try:\n        return json.loads(json_string)\n    except json.JSONDecodeError:\n        return return_type()\n</code></pre>"},{"location":"reference/documentation/core/utils/#neurotrace.core.utils.strip_json_code_block","title":"<code>strip_json_code_block(text)</code>","text":"<p>Strip code block formatting from a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text containing a JSON code block.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned JSON string without code block formatting.</p> Source code in <code>neurotrace/core/utils.py</code> <pre><code>def strip_json_code_block(text: str) -&gt; str:\n    \"\"\"\n    Strip code block formatting from a JSON string.\n\n    Args:\n        text (str): The input text containing a JSON code block.\n\n    Returns:\n        str: The cleaned JSON string without code block formatting.\n    \"\"\"\n    return text.strip(\"```json\\n\").strip(\"```\").strip()\n</code></pre>"},{"location":"reference/documentation/core/vector_memory/","title":"<code>neurotrace.core.vector_memory</code>","text":""},{"location":"reference/documentation/core/vector_memory/#neurotrace.core.vector_memory","title":"<code>neurotrace.core.vector_memory</code>","text":""},{"location":"reference/documentation/core/vector_memory/#neurotrace.core.vector_memory.BaseVectorMemoryAdapter","title":"<code>BaseVectorMemoryAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for vector memory storage adapters.</p> <p>This class defines the interface for storing and retrieving messages using vector embeddings. Implementations should provide concrete methods for adding, searching, and optionally deleting messages from the vector store.</p> Source code in <code>neurotrace/core/vector_memory.py</code> <pre><code>class BaseVectorMemoryAdapter(ABC):\n    \"\"\"Abstract base class for vector memory storage adapters.\n\n    This class defines the interface for storing and retrieving messages using\n    vector embeddings. Implementations should provide concrete methods for\n    adding, searching, and optionally deleting messages from the vector store.\n    \"\"\"\n\n    @abstractmethod\n    def add_messages(self, messages: List[Message]) -&gt; None:\n        \"\"\"Add a list of messages to the vector memory store.\n\n        Args:\n            messages (List[Message]): List of Message objects to be added to\n                the vector store. Each message will be embedded and stored.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query: str, k: int = 5) -&gt; List[Message]:\n        \"\"\"Search the vector memory for the most relevant messages.\n\n        Performs a similarity search in the vector store using the provided\n        query string. The query will be embedded and compared against stored\n        message embeddings.\n\n        Args:\n            query (str): The search query string.\n            k (int, optional): Maximum number of results to return. Defaults to 5.\n\n        Returns:\n            List[Message]: List of messages ranked by similarity to the query,\n                limited to k results.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete(self, ids: List[str]) -&gt; None:\n        \"\"\"Delete messages from the vector store by their IDs.\n\n        Args:\n            ids (List[str]): List of message IDs to be deleted from the store.\n\n        Note:\n            This is an optional operation that might not be supported by all\n            vector store implementations.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/documentation/core/vector_memory/#neurotrace.core.vector_memory.BaseVectorMemoryAdapter.add_messages","title":"<code>add_messages(messages)</code>  <code>abstractmethod</code>","text":"<p>Add a list of messages to the vector memory store.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>List of Message objects to be added to the vector store. Each message will be embedded and stored.</p> required Source code in <code>neurotrace/core/vector_memory.py</code> <pre><code>@abstractmethod\ndef add_messages(self, messages: List[Message]) -&gt; None:\n    \"\"\"Add a list of messages to the vector memory store.\n\n    Args:\n        messages (List[Message]): List of Message objects to be added to\n            the vector store. Each message will be embedded and stored.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/documentation/core/vector_memory/#neurotrace.core.vector_memory.BaseVectorMemoryAdapter.delete","title":"<code>delete(ids)</code>  <code>abstractmethod</code>","text":"<p>Delete messages from the vector store by their IDs.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[str]</code> <p>List of message IDs to be deleted from the store.</p> required Note <p>This is an optional operation that might not be supported by all vector store implementations.</p> Source code in <code>neurotrace/core/vector_memory.py</code> <pre><code>@abstractmethod\ndef delete(self, ids: List[str]) -&gt; None:\n    \"\"\"Delete messages from the vector store by their IDs.\n\n    Args:\n        ids (List[str]): List of message IDs to be deleted from the store.\n\n    Note:\n        This is an optional operation that might not be supported by all\n        vector store implementations.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/documentation/core/vector_memory/#neurotrace.core.vector_memory.BaseVectorMemoryAdapter.search","title":"<code>search(query, k=5)</code>  <code>abstractmethod</code>","text":"<p>Search the vector memory for the most relevant messages.</p> <p>Performs a similarity search in the vector store using the provided query string. The query will be embedded and compared against stored message embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query string.</p> required <code>k</code> <code>int</code> <p>Maximum number of results to return. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>List[Message]: List of messages ranked by similarity to the query, limited to k results.</p> Source code in <code>neurotrace/core/vector_memory.py</code> <pre><code>@abstractmethod\ndef search(self, query: str, k: int = 5) -&gt; List[Message]:\n    \"\"\"Search the vector memory for the most relevant messages.\n\n    Performs a similarity search in the vector store using the provided\n    query string. The query will be embedded and compared against stored\n    message embeddings.\n\n    Args:\n        query (str): The search query string.\n        k (int, optional): Maximum number of results to return. Defaults to 5.\n\n    Returns:\n        List[Message]: List of messages ranked by similarity to the query,\n            limited to k results.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/documentation/core/vector_memory/#neurotrace.core.vector_memory.VectorMemoryAdapter","title":"<code>VectorMemoryAdapter</code>","text":"<p>               Bases: <code>BaseVectorMemoryAdapter</code></p> <p>Concrete implementation of vector memory storage using LangChain components.</p> <p>This adapter wraps a LangChain-compatible vector store and embedding model to provide vector-based message storage and retrieval.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>VectorStore</code> <p>LangChain vector store implementation for storing embeddings.</p> required <code>embedding_model</code> <code>Embeddings</code> <p>LangChain embeddings model for converting text to vectors.</p> required Source code in <code>neurotrace/core/vector_memory.py</code> <pre><code>class VectorMemoryAdapter(BaseVectorMemoryAdapter):\n    \"\"\"Concrete implementation of vector memory storage using LangChain components.\n\n    This adapter wraps a LangChain-compatible vector store and embedding model\n    to provide vector-based message storage and retrieval.\n\n    Args:\n        vector_store (VectorStore): LangChain vector store implementation for\n            storing embeddings.\n        embedding_model (Embeddings): LangChain embeddings model for converting\n            text to vectors.\n    \"\"\"\n\n    def __init__(self, vector_store: VectorStore):\n        \"\"\"\n        Vector memory adapter that wraps a LangChain-compatible vector store.\n\n        Args:\n            vector_store (VectorStore): Any LangChain-compatible vector store.\n            embedding_model (Embeddings): Embedding model to generate embeddings.\n        \"\"\"\n        self.vector_store = vector_store\n        self.embedding_model = vector_store.embeddings\n\n    def add_messages(self, messages: List[Message]) -&gt; None:\n        \"\"\"Add multiple messages to the vector store.\n\n        Converts the messages to LangChain Document format and adds them to\n        the underlying vector store. The documents will be automatically\n        embedded using the configured embedding model.\n\n        Args:\n            messages (List[Message]): List of messages to be added to the\n                vector store.\n        \"\"\"\n        documents = [msg.to_document() for msg in messages]\n        self.vector_store.add_documents(documents)\n\n    def search(self, query: str, k: int = 5) -&gt; List[Message]:\n        \"\"\"Search for similar messages in the vector store.\n\n        Performs a similarity search using the query string. The query is\n        embedded and compared against stored message embeddings to find\n        the most similar messages.\n\n        Args:\n            query (str): The search query string.\n            k (int, optional): Maximum number of results to return. Defaults to 5.\n\n        Returns:\n            List[Message]: List of messages ranked by similarity to the query,\n                limited to k results.\n\n        Note:\n            TODO: Add support for enhancing the prompt for vector search using LLM.\n        \"\"\"\n        # todo: add support for enhancing the prompt for vector search using llm\n        results = self.vector_store.similarity_search(query=query, k=k)\n        return [Message.from_document(doc) for doc in results]\n\n    def delete(self, ids: List[str]) -&gt; None:\n        \"\"\"Delete messages from the vector store by their IDs.\n\n        Args:\n            ids (List[str]): List of message IDs to be deleted.\n\n        Raises:\n            NotImplementedError: If the underlying vector store doesn't\n                support deletion operations.\n        \"\"\"\n        if hasattr(self.vector_store, \"delete\"):\n            self.vector_store.delete(ids)\n        else:\n            raise NotImplementedError(f\"Delete not supported by {type(self.vector_store)}.\")\n</code></pre>"},{"location":"reference/documentation/core/vector_memory/#neurotrace.core.vector_memory.VectorMemoryAdapter.__init__","title":"<code>__init__(vector_store)</code>","text":"<p>Vector memory adapter that wraps a LangChain-compatible vector store.</p> <p>Parameters:</p> Name Type Description Default <code>vector_store</code> <code>VectorStore</code> <p>Any LangChain-compatible vector store.</p> required <code>embedding_model</code> <code>Embeddings</code> <p>Embedding model to generate embeddings.</p> required Source code in <code>neurotrace/core/vector_memory.py</code> <pre><code>def __init__(self, vector_store: VectorStore):\n    \"\"\"\n    Vector memory adapter that wraps a LangChain-compatible vector store.\n\n    Args:\n        vector_store (VectorStore): Any LangChain-compatible vector store.\n        embedding_model (Embeddings): Embedding model to generate embeddings.\n    \"\"\"\n    self.vector_store = vector_store\n    self.embedding_model = vector_store.embeddings\n</code></pre>"},{"location":"reference/documentation/core/vector_memory/#neurotrace.core.vector_memory.VectorMemoryAdapter.add_messages","title":"<code>add_messages(messages)</code>","text":"<p>Add multiple messages to the vector store.</p> <p>Converts the messages to LangChain Document format and adds them to the underlying vector store. The documents will be automatically embedded using the configured embedding model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>List of messages to be added to the vector store.</p> required Source code in <code>neurotrace/core/vector_memory.py</code> <pre><code>def add_messages(self, messages: List[Message]) -&gt; None:\n    \"\"\"Add multiple messages to the vector store.\n\n    Converts the messages to LangChain Document format and adds them to\n    the underlying vector store. The documents will be automatically\n    embedded using the configured embedding model.\n\n    Args:\n        messages (List[Message]): List of messages to be added to the\n            vector store.\n    \"\"\"\n    documents = [msg.to_document() for msg in messages]\n    self.vector_store.add_documents(documents)\n</code></pre>"},{"location":"reference/documentation/core/vector_memory/#neurotrace.core.vector_memory.VectorMemoryAdapter.delete","title":"<code>delete(ids)</code>","text":"<p>Delete messages from the vector store by their IDs.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[str]</code> <p>List of message IDs to be deleted.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the underlying vector store doesn't support deletion operations.</p> Source code in <code>neurotrace/core/vector_memory.py</code> <pre><code>def delete(self, ids: List[str]) -&gt; None:\n    \"\"\"Delete messages from the vector store by their IDs.\n\n    Args:\n        ids (List[str]): List of message IDs to be deleted.\n\n    Raises:\n        NotImplementedError: If the underlying vector store doesn't\n            support deletion operations.\n    \"\"\"\n    if hasattr(self.vector_store, \"delete\"):\n        self.vector_store.delete(ids)\n    else:\n        raise NotImplementedError(f\"Delete not supported by {type(self.vector_store)}.\")\n</code></pre>"},{"location":"reference/documentation/core/vector_memory/#neurotrace.core.vector_memory.VectorMemoryAdapter.search","title":"<code>search(query, k=5)</code>","text":"<p>Search for similar messages in the vector store.</p> <p>Performs a similarity search using the query string. The query is embedded and compared against stored message embeddings to find the most similar messages.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query string.</p> required <code>k</code> <code>int</code> <p>Maximum number of results to return. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>List[Message]: List of messages ranked by similarity to the query, limited to k results.</p> Note <p>TODO: Add support for enhancing the prompt for vector search using LLM.</p> Source code in <code>neurotrace/core/vector_memory.py</code> <pre><code>def search(self, query: str, k: int = 5) -&gt; List[Message]:\n    \"\"\"Search for similar messages in the vector store.\n\n    Performs a similarity search using the query string. The query is\n    embedded and compared against stored message embeddings to find\n    the most similar messages.\n\n    Args:\n        query (str): The search query string.\n        k (int, optional): Maximum number of results to return. Defaults to 5.\n\n    Returns:\n        List[Message]: List of messages ranked by similarity to the query,\n            limited to k results.\n\n    Note:\n        TODO: Add support for enhancing the prompt for vector search using LLM.\n    \"\"\"\n    # todo: add support for enhancing the prompt for vector search using llm\n    results = self.vector_store.similarity_search(query=query, k=k)\n    return [Message.from_document(doc) for doc in results]\n</code></pre>"},{"location":"reference/documentation/core/adapters/langchain_adapter/","title":"<code>neurotrace.core.adapters.langchain_adapter</code>","text":""},{"location":"reference/documentation/core/adapters/langchain_adapter/#neurotrace.core.adapters.langchain_adapter","title":"<code>neurotrace.core.adapters.langchain_adapter</code>","text":"<p>LangChain Adapter Module.</p> <p>This module provides adapter functions for converting between neurotrace's internal Message format and LangChain's message formats. It handles bidirectional conversion between neurotrace Messages and LangChain's HumanMessage/AIMessage types.</p>"},{"location":"reference/documentation/core/adapters/langchain_adapter/#neurotrace.core.adapters.langchain_adapter.from_langchain_message","title":"<code>from_langchain_message(msg, role=None)</code>","text":"<p>Convert a LangChain message to a neurotrace Message.</p> <p>This function transforms a LangChain message into neurotrace's Message format, with automatic role detection based on the message type or an optional explicit role override.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>BaseMessage</code> <p>The LangChain message to convert.</p> required <code>role</code> <code>Optional[str]</code> <p>Explicitly specify the role to use. If None, role is detected from the message type. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Message</code> <code>Message</code> <p>A neurotrace Message with: - role determined by message type or override - content from the original message</p> Example <p>lc_msg = HumanMessage(content=\"Hello\") msg = from_langchain_message(lc_msg) print(msg.role)  # \"user\" print(msg.content)  # \"Hello\"</p> Source code in <code>neurotrace/core/adapters/langchain_adapter.py</code> <pre><code>def from_langchain_message(msg: BaseMessage, role: Optional[str] = None) -&gt; Message:\n    \"\"\"\n    Convert a LangChain message to a neurotrace Message.\n\n    This function transforms a LangChain message into neurotrace's Message format,\n    with automatic role detection based on the message type or an optional\n    explicit role override.\n\n    Args:\n        msg (BaseMessage): The LangChain message to convert.\n        role (Optional[str], optional): Explicitly specify the role to use.\n            If None, role is detected from the message type. Defaults to None.\n\n    Returns:\n        Message: A neurotrace Message with:\n            - role determined by message type or override\n            - content from the original message\n\n    Example:\n        &gt;&gt;&gt; lc_msg = HumanMessage(content=\"Hello\")\n        &gt;&gt;&gt; msg = from_langchain_message(lc_msg)\n        &gt;&gt;&gt; print(msg.role)  # \"user\"\n        &gt;&gt;&gt; print(msg.content)  # \"Hello\"\n    \"\"\"\n    detected_role = (\n        role if role else\n        \"human\" if isinstance(msg, HumanMessage) else\n        \"ai\" if isinstance(msg, AIMessage) else\n        \"system\"\n    )\n\n    role_literal = cast(Literal[\"user\", \"ai\", \"system\"], detected_role)\n    return Message(\n        role=role_literal,\n        content=msg.content\n    )\n</code></pre>"},{"location":"reference/documentation/core/adapters/vector_db_adapter/","title":"<code>neurotrace.core.adapters.vector_db_adapter</code>","text":""},{"location":"reference/documentation/core/adapters/vector_db_adapter/#neurotrace.core.adapters.vector_db_adapter","title":"<code>neurotrace.core.adapters.vector_db_adapter</code>","text":"<p>Vector Database Adapter Module.</p> <p>This module provides adapter functions for converting between neurotrace's internal message format and vector database record format. It handles the serialization of Message objects into a format suitable for vector database storage and retrieval.</p>"},{"location":"reference/documentation/core/adapters/vector_db_adapter/#neurotrace.core.adapters.vector_db_adapter.to_vector_record","title":"<code>to_vector_record(msg)</code>","text":"<p>Converts a Message object to a vector database record format.</p> <p>This function transforms a neurotrace Message instance into a dictionary format suitable for storage in a vector database. It extracts essential fields including the message ID, content text, embedding vector, and all associated metadata.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Message</code> <p>The Message object to convert.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing: - id: The message's unique identifier - text: The message content - embedding: The message's vector embedding - metadata: All additional metadata as a dictionary</p> Example <p>message = Message(id=\"123\", content=\"Hello\", metadata=MessageMetadata(...)) record = to_vector_record(message) print(record) {     'id': '123',     'text': 'Hello',     'embedding': [...],     'metadata': {...} }</p> Source code in <code>neurotrace/core/adapters/vector_db_adapter.py</code> <pre><code>def to_vector_record(msg: Message) -&gt; Dict[str, Any]:\n    \"\"\"\n    Converts a Message object to a vector database record format.\n\n    This function transforms a neurotrace Message instance into a dictionary format\n    suitable for storage in a vector database. It extracts essential fields including\n    the message ID, content text, embedding vector, and all associated metadata.\n\n    Args:\n        msg (Message): The Message object to convert.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing:\n            - id: The message's unique identifier\n            - text: The message content\n            - embedding: The message's vector embedding\n            - metadata: All additional metadata as a dictionary\n\n    Example:\n        &gt;&gt;&gt; message = Message(id=\"123\", content=\"Hello\", metadata=MessageMetadata(...))\n        &gt;&gt;&gt; record = to_vector_record(message)\n        &gt;&gt;&gt; print(record)\n        {\n            'id': '123',\n            'text': 'Hello',\n            'embedding': [...],\n            'metadata': {...}\n        }\n    \"\"\"\n    return {\n        \"id\": msg.id,\n        \"text\": msg.content,\n        \"embedding\": msg.metadata.embedding,\n        \"metadata\": msg.metadata.model_dump()\n    }\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/","title":"<code>neurotrace.core.hippocampus.ltm</code>","text":""},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm","title":"<code>neurotrace.core.hippocampus.ltm</code>","text":""},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.BaseLongTermMemory","title":"<code>BaseLongTermMemory</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for long-term memory storage.</p> <p>This class defines the interface for persistent storage of conversation messages. Implementations should handle the storage and retrieval of messages across different chat sessions.</p> Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>class BaseLongTermMemory(ABC):\n    \"\"\"Abstract base class for long-term memory storage.\n\n    This class defines the interface for persistent storage of conversation\n    messages. Implementations should handle the storage and retrieval of\n    messages across different chat sessions.\n    \"\"\"\n\n    @abstractmethod\n    def add_message(self, message: Message) -&gt; None:\n        \"\"\"Store a message in long-term memory.\n\n        Args:\n            message (Message): The message object to be stored.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_user_message(self, content: str) -&gt; None:\n        \"\"\"Add a user message to long-term memory.\n\n        Args:\n            content (str): The content of the user's message.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_ai_message(self, content: str) -&gt; None:\n        \"\"\"Add an AI message to long-term memory.\n\n        Args:\n            content (str): The content of the AI's message.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_messages(self, session_id: str) -&gt; List[Message]:\n        \"\"\"Retrieve all messages for a given session.\n\n        Args:\n            session_id (str): The identifier for the chat session.\n\n        Returns:\n            List[Message]: List of messages associated with the session.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def clear(self, session_id: str) -&gt; None:\n        \"\"\"Clear messages for a given session.\n\n        Args:\n            session_id (str): The identifier for the chat session to clear.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.BaseLongTermMemory.add_ai_message","title":"<code>add_ai_message(content)</code>  <code>abstractmethod</code>","text":"<p>Add an AI message to long-term memory.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the AI's message.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>@abstractmethod\ndef add_ai_message(self, content: str) -&gt; None:\n    \"\"\"Add an AI message to long-term memory.\n\n    Args:\n        content (str): The content of the AI's message.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.BaseLongTermMemory.add_message","title":"<code>add_message(message)</code>  <code>abstractmethod</code>","text":"<p>Store a message in long-term memory.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The message object to be stored.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>@abstractmethod\ndef add_message(self, message: Message) -&gt; None:\n    \"\"\"Store a message in long-term memory.\n\n    Args:\n        message (Message): The message object to be stored.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.BaseLongTermMemory.add_user_message","title":"<code>add_user_message(content)</code>  <code>abstractmethod</code>","text":"<p>Add a user message to long-term memory.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the user's message.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>@abstractmethod\ndef add_user_message(self, content: str) -&gt; None:\n    \"\"\"Add a user message to long-term memory.\n\n    Args:\n        content (str): The content of the user's message.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.BaseLongTermMemory.clear","title":"<code>clear(session_id)</code>  <code>abstractmethod</code>","text":"<p>Clear messages for a given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The identifier for the chat session to clear.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>@abstractmethod\ndef clear(self, session_id: str) -&gt; None:\n    \"\"\"Clear messages for a given session.\n\n    Args:\n        session_id (str): The identifier for the chat session to clear.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.BaseLongTermMemory.get_messages","title":"<code>get_messages(session_id)</code>  <code>abstractmethod</code>","text":"<p>Retrieve all messages for a given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The identifier for the chat session.</p> required <p>Returns:</p> Type Description <code>List[Message]</code> <p>List[Message]: List of messages associated with the session.</p> Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>@abstractmethod\ndef get_messages(self, session_id: str) -&gt; List[Message]:\n    \"\"\"Retrieve all messages for a given session.\n\n    Args:\n        session_id (str): The identifier for the chat session.\n\n    Returns:\n        List[Message]: List of messages associated with the session.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory","title":"<code>LongTermMemory</code>","text":"<p>               Bases: <code>BaseLongTermMemory</code></p> <p>LangChain chat history adapter for long-term memory storage.</p> <p>This adapter implements the BaseLongTermMemory interface using LangChain's chat history components for persistent storage.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>BaseChatMessageHistory</code> <p>LangChain chat history implementation to use for storage.</p> required <code>session_id</code> <code>str</code> <p>Default session identifier. Defaults to \"default\".</p> <code>'default'</code> Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>class LongTermMemory(BaseLongTermMemory):\n    \"\"\"LangChain chat history adapter for long-term memory storage.\n\n    This adapter implements the BaseLongTermMemory interface using LangChain's\n    chat history components for persistent storage.\n\n    Args:\n        history (BaseChatMessageHistory): LangChain chat history implementation\n            to use for storage.\n        session_id (str, optional): Default session identifier. Defaults to \"default\".\n    \"\"\"\n\n    def __init__(self, history: BaseChatMessageHistory, session_id: str = \"default\"):\n        \"\"\"Initialize the LangChain history adapter.\n\n        Args:\n            history (BaseChatMessageHistory): LangChain chat history implementation.\n            session_id (str, optional): Default session identifier. Defaults to \"default\".\n        \"\"\"\n        self.history = history\n        self.session_id = session_id\n\n    def add_message(self, message: Message) -&gt; None:\n        \"\"\"Add a message to the LangChain chat history.\n\n        Converts the Message object to LangChain's message format before storing.\n\n        Args:\n            message (Message): The message to store.\n        \"\"\"\n        lc_msg: BaseMessage = message.to_langchain_message()\n        self.history.add_message(lc_msg)\n\n    def add_user_message(self, content: str) -&gt; None:\n        \"\"\"Add a user message to the chat history.\n\n        Creates a Message object with HUMAN role and adds it to storage.\n\n        Args:\n            content (str): The content of the user's message.\n        \"\"\"\n        self.add_message(Message(role=Role.HUMAN.value, content=content))\n\n    def add_ai_message(self, content: str) -&gt; None:\n        \"\"\"Add an AI message to the chat history.\n\n        Creates a Message object with AI role and adds it to storage.\n\n        Args:\n            content (str): The content of the AI's message.\n        \"\"\"\n        self.add_message(Message(role=Role.AI.value, content=content))\n\n    def get_messages(self, session_id: str = None) -&gt; List[Message]:\n        \"\"\"Retrieve all messages from the chat history.\n\n        Args:\n            session_id (str, optional): Session identifier. Currently unused as\n                LangChain history doesn't support session filtering. Defaults to None.\n\n        Returns:\n            List[Message]: All messages in the chat history.\n        \"\"\"\n        lc_msgs = self.history.messages\n        return [from_langchain_message(m) for m in lc_msgs]\n\n    def clear(self, session_id: str = None) -&gt; None:\n        \"\"\"Clear all messages from the chat history.\n\n        Args:\n            session_id (str, optional): Session identifier. Currently unused as\n                LangChain history doesn't support session filtering. Defaults to None.\n        \"\"\"\n        self.history.clear()\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory.__init__","title":"<code>__init__(history, session_id='default')</code>","text":"<p>Initialize the LangChain history adapter.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>BaseChatMessageHistory</code> <p>LangChain chat history implementation.</p> required <code>session_id</code> <code>str</code> <p>Default session identifier. Defaults to \"default\".</p> <code>'default'</code> Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>def __init__(self, history: BaseChatMessageHistory, session_id: str = \"default\"):\n    \"\"\"Initialize the LangChain history adapter.\n\n    Args:\n        history (BaseChatMessageHistory): LangChain chat history implementation.\n        session_id (str, optional): Default session identifier. Defaults to \"default\".\n    \"\"\"\n    self.history = history\n    self.session_id = session_id\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory.add_ai_message","title":"<code>add_ai_message(content)</code>","text":"<p>Add an AI message to the chat history.</p> <p>Creates a Message object with AI role and adds it to storage.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the AI's message.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>def add_ai_message(self, content: str) -&gt; None:\n    \"\"\"Add an AI message to the chat history.\n\n    Creates a Message object with AI role and adds it to storage.\n\n    Args:\n        content (str): The content of the AI's message.\n    \"\"\"\n    self.add_message(Message(role=Role.AI.value, content=content))\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory.add_message","title":"<code>add_message(message)</code>","text":"<p>Add a message to the LangChain chat history.</p> <p>Converts the Message object to LangChain's message format before storing.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The message to store.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>def add_message(self, message: Message) -&gt; None:\n    \"\"\"Add a message to the LangChain chat history.\n\n    Converts the Message object to LangChain's message format before storing.\n\n    Args:\n        message (Message): The message to store.\n    \"\"\"\n    lc_msg: BaseMessage = message.to_langchain_message()\n    self.history.add_message(lc_msg)\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory.add_user_message","title":"<code>add_user_message(content)</code>","text":"<p>Add a user message to the chat history.</p> <p>Creates a Message object with HUMAN role and adds it to storage.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the user's message.</p> required Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>def add_user_message(self, content: str) -&gt; None:\n    \"\"\"Add a user message to the chat history.\n\n    Creates a Message object with HUMAN role and adds it to storage.\n\n    Args:\n        content (str): The content of the user's message.\n    \"\"\"\n    self.add_message(Message(role=Role.HUMAN.value, content=content))\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory.clear","title":"<code>clear(session_id=None)</code>","text":"<p>Clear all messages from the chat history.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Session identifier. Currently unused as LangChain history doesn't support session filtering. Defaults to None.</p> <code>None</code> Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>def clear(self, session_id: str = None) -&gt; None:\n    \"\"\"Clear all messages from the chat history.\n\n    Args:\n        session_id (str, optional): Session identifier. Currently unused as\n            LangChain history doesn't support session filtering. Defaults to None.\n    \"\"\"\n    self.history.clear()\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/ltm/#neurotrace.core.hippocampus.ltm.LongTermMemory.get_messages","title":"<code>get_messages(session_id=None)</code>","text":"<p>Retrieve all messages from the chat history.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Session identifier. Currently unused as LangChain history doesn't support session filtering. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>List[Message]: All messages in the chat history.</p> Source code in <code>neurotrace/core/hippocampus/ltm.py</code> <pre><code>def get_messages(self, session_id: str = None) -&gt; List[Message]:\n    \"\"\"Retrieve all messages from the chat history.\n\n    Args:\n        session_id (str, optional): Session identifier. Currently unused as\n            LangChain history doesn't support session filtering. Defaults to None.\n\n    Returns:\n        List[Message]: All messages in the chat history.\n    \"\"\"\n    lc_msgs = self.history.messages\n    return [from_langchain_message(m) for m in lc_msgs]\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/memory_orchestrator/","title":"<code>neurotrace.core.hippocampus.memory_orchestrator</code>","text":""},{"location":"reference/documentation/core/hippocampus/memory_orchestrator/#neurotrace.core.hippocampus.memory_orchestrator","title":"<code>neurotrace.core.hippocampus.memory_orchestrator</code>","text":""},{"location":"reference/documentation/core/hippocampus/memory_orchestrator/#neurotrace.core.hippocampus.memory_orchestrator.MemoryOrchestrator","title":"<code>MemoryOrchestrator</code>","text":"<p>Manages both short-term and long-term memory for Neurotrace agents.</p> Source code in <code>neurotrace/core/hippocampus/memory_orchestrator.py</code> <pre><code>class MemoryOrchestrator:\n    \"\"\"Manages both short-term and long-term memory for Neurotrace agents.\"\"\"\n\n    def __init__(\n        self,\n        llm: Union[BaseLLM, BaseChatModel],\n        graph_store: GraphStore,\n        vector_store: VectorStore,\n    ):\n        self.llm = llm\n        self._graph_indexer = GraphTripletIndexer(llm)\n        self._graph_memory_adapter = GraphMemoryAdapter(llm, graph_store, self._graph_indexer)\n        self._vector_memory_adapter = VectorMemoryAdapter(vector_store)\n\n    def save_in_graph_memory(self, summary: str, tags: List[str] = None) -&gt; str:\n        \"\"\"Saves a summary in graph memory.\"\"\"\n        # todo: interface this with Message class\n        self._graph_memory_adapter.add_conversation(summary, tags=tags)\n        return \"Graph memory saved.\"\n\n    def save_in_vector_memory(self, summary: str, tags: List[str] = None) -&gt; str:\n        \"\"\"Saves a summary in vector memory.\"\"\"\n        message = Message(role=Role.AI.value, content=summary, metadata=MessageMetadata(tags=tags))\n        self._vector_memory_adapter.add_messages([message])\n        return \"Vector memory saved.\"\n\n    def search_vector_memory(self, query: str, k: int = 5) -&gt; List[Message]:\n        return self._vector_memory_adapter.search(query, k)\n\n    def search_graph_memory(self, query: str) -&gt; str:\n        return self._graph_memory_adapter.ask_graph(query)[\"result\"]\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/memory_orchestrator/#neurotrace.core.hippocampus.memory_orchestrator.MemoryOrchestrator.save_in_graph_memory","title":"<code>save_in_graph_memory(summary, tags=None)</code>","text":"<p>Saves a summary in graph memory.</p> Source code in <code>neurotrace/core/hippocampus/memory_orchestrator.py</code> <pre><code>def save_in_graph_memory(self, summary: str, tags: List[str] = None) -&gt; str:\n    \"\"\"Saves a summary in graph memory.\"\"\"\n    # todo: interface this with Message class\n    self._graph_memory_adapter.add_conversation(summary, tags=tags)\n    return \"Graph memory saved.\"\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/memory_orchestrator/#neurotrace.core.hippocampus.memory_orchestrator.MemoryOrchestrator.save_in_vector_memory","title":"<code>save_in_vector_memory(summary, tags=None)</code>","text":"<p>Saves a summary in vector memory.</p> Source code in <code>neurotrace/core/hippocampus/memory_orchestrator.py</code> <pre><code>def save_in_vector_memory(self, summary: str, tags: List[str] = None) -&gt; str:\n    \"\"\"Saves a summary in vector memory.\"\"\"\n    message = Message(role=Role.AI.value, content=summary, metadata=MessageMetadata(tags=tags))\n    self._vector_memory_adapter.add_messages([message])\n    return \"Vector memory saved.\"\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/","title":"<code>neurotrace.core.hippocampus.stm</code>","text":""},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm","title":"<code>neurotrace.core.hippocampus.stm</code>","text":""},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory","title":"<code>BaseShortTermMemory</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for short-term memory management.</p> <p>This class defines the interface for managing a temporary message store with token limit constraints.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>class BaseShortTermMemory(ABC):\n    \"\"\"Abstract base class for short-term memory management.\n\n    This class defines the interface for managing a temporary message store\n    with token limit constraints.\n    \"\"\"\n\n    @abstractmethod\n    def append(self, message: Message) -&gt; None:\n        \"\"\"Add a message to short-term memory.\n\n        Args:\n            message (Message): The message to be added to memory.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_messages(self) -&gt; List[Message]:\n        \"\"\"Retrieve all messages from short-term memory.\n\n        Returns:\n            List[Message]: List of all messages currently in memory.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"Remove all messages from short-term memory.\"\"\"\n        ...\n\n    @abstractmethod\n    def set_messages(self, messages: List[Message]) -&gt; None:\n        \"\"\"Replace all messages in memory with a new list.\n\n        Args:\n            messages (List[Message]): New list of messages to store.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def total_tokens(self) -&gt; int:\n        \"\"\"Calculate total tokens used by all messages.\n\n        Returns:\n            int: Sum of estimated token lengths of all messages.\n        \"\"\"\n        ...\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the number of messages in memory.\n\n        Returns:\n            int: Count of messages currently stored.\n        \"\"\"\n        return len(self.get_messages())\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of messages in memory.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Count of messages currently stored.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of messages in memory.\n\n    Returns:\n        int: Count of messages currently stored.\n    \"\"\"\n    return len(self.get_messages())\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory.append","title":"<code>append(message)</code>  <code>abstractmethod</code>","text":"<p>Add a message to short-term memory.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The message to be added to memory.</p> required Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>@abstractmethod\ndef append(self, message: Message) -&gt; None:\n    \"\"\"Add a message to short-term memory.\n\n    Args:\n        message (Message): The message to be added to memory.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>Remove all messages from short-term memory.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"Remove all messages from short-term memory.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory.get_messages","title":"<code>get_messages()</code>  <code>abstractmethod</code>","text":"<p>Retrieve all messages from short-term memory.</p> <p>Returns:</p> Type Description <code>List[Message]</code> <p>List[Message]: List of all messages currently in memory.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>@abstractmethod\ndef get_messages(self) -&gt; List[Message]:\n    \"\"\"Retrieve all messages from short-term memory.\n\n    Returns:\n        List[Message]: List of all messages currently in memory.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory.set_messages","title":"<code>set_messages(messages)</code>  <code>abstractmethod</code>","text":"<p>Replace all messages in memory with a new list.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>New list of messages to store.</p> required Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>@abstractmethod\ndef set_messages(self, messages: List[Message]) -&gt; None:\n    \"\"\"Replace all messages in memory with a new list.\n\n    Args:\n        messages (List[Message]): New list of messages to store.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.BaseShortTermMemory.total_tokens","title":"<code>total_tokens()</code>  <code>abstractmethod</code>","text":"<p>Calculate total tokens used by all messages.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Sum of estimated token lengths of all messages.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>@abstractmethod\ndef total_tokens(self) -&gt; int:\n    \"\"\"Calculate total tokens used by all messages.\n\n    Returns:\n        int: Sum of estimated token lengths of all messages.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory","title":"<code>ShortTermMemory</code>","text":"<p>               Bases: <code>BaseShortTermMemory</code></p> <p>Implementation of token-limited short-term memory.</p> <p>This class maintains a list of messages while ensuring the total token count stays within a specified limit. When the limit is exceeded, older messages are automatically evicted.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to store. Set to 0 to disable memory (all messages will be evicted). Defaults to 2048.</p> <code>2048</code> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>class ShortTermMemory(BaseShortTermMemory):\n    \"\"\"Implementation of token-limited short-term memory.\n\n    This class maintains a list of messages while ensuring the total token\n    count stays within a specified limit. When the limit is exceeded, older\n    messages are automatically evicted.\n\n    Args:\n        max_tokens (int, optional): Maximum number of tokens to store. Set to 0\n            to disable memory (all messages will be evicted). Defaults to 2048.\n    \"\"\"\n\n    def __init__(self, max_tokens: int = 2048):\n        \"\"\"Initialize short-term memory with token limit.\n\n        Args:\n            max_tokens (int, optional): Maximum number of tokens to store.\n                Defaults to 2048.\n        \"\"\"\n        self.messages: List[Message] = []\n        self.max_tokens = max_tokens\n\n    def append(self, message: Message) -&gt; None:\n        \"\"\"Add a message to memory and evict old messages if needed.\n\n        If the message doesn't have an ID, generates a UUID for it. After\n        adding the message, ensures token limit compliance by evicting old\n        messages if necessary.\n\n        Args:\n            message (Message): The message to add to memory.\n        \"\"\"\n        if not message.id:\n            message.id = str(uuid.uuid4())\n\n        self.messages.append(message)\n        MemoryLogger.log_add(message, destination=\"stm\")\n        self._evict_if_needed()\n\n    def get_messages(self) -&gt; List[Message]:\n        \"\"\"Get all messages currently in memory.\n\n        Returns:\n            List[Message]: List of all stored messages.\n        \"\"\"\n        return self.messages\n\n    def clear(self) -&gt; None:\n        \"\"\"Remove all messages from memory.\"\"\"\n        self.messages = []\n\n    def _evict_if_needed(self) -&gt; None:\n        \"\"\"Maintain token limit by removing oldest messages.\n\n        If max_tokens is 0, clears all messages. Otherwise, removes oldest\n        messages until total token count is within limit, always keeping at\n        least one message.\n        \"\"\"\n        # If max_tokens is 0, clear everything (user wants no memory)\n        if self.max_tokens == 0:\n            self.messages.clear()\n            return\n\n        total = sum(msg.estimated_token_length() for msg in self.messages)\n\n        # Keep at least 1 message even if over limit (unless max_tokens is zero)\n        while total &gt; self.max_tokens and len(self.messages) &gt; 1:\n            evicted = self.messages.pop(0)\n            total -= evicted.estimated_token_length()\n            MemoryLogger.log_evict(evicted)\n\n    def set_messages(self, messages: List[Message]) -&gt; None:\n        \"\"\"Replace current messages with new list and maintain token limit.\n\n        Args:\n            messages (List[Message]): New messages to store in memory.\n        \"\"\"\n        self.messages = messages\n        self._evict_if_needed()\n\n    def total_tokens(self) -&gt; int:\n        \"\"\"Calculate total tokens used by all messages.\n\n        Returns:\n            int: Sum of estimated token lengths across all messages.\n        \"\"\"\n        return sum(m.estimated_token_length() for m in self.messages)\n\n    def __len__(self):\n        \"\"\"Get number of messages in memory.\n\n        Returns:\n            int: Count of stored messages.\n        \"\"\"\n        return len(self.messages)\n\n    def __repr__(self):\n        \"\"\"Get string representation of memory state.\n\n        Returns:\n            str: String showing message count and token usage/limit.\n        \"\"\"\n        return f\"&lt;STM messages={len(self.messages)} tokens={self.total_tokens()}/{self.max_tokens}&gt;\"\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.__init__","title":"<code>__init__(max_tokens=2048)</code>","text":"<p>Initialize short-term memory with token limit.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to store. Defaults to 2048.</p> <code>2048</code> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def __init__(self, max_tokens: int = 2048):\n    \"\"\"Initialize short-term memory with token limit.\n\n    Args:\n        max_tokens (int, optional): Maximum number of tokens to store.\n            Defaults to 2048.\n    \"\"\"\n    self.messages: List[Message] = []\n    self.max_tokens = max_tokens\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.__len__","title":"<code>__len__()</code>","text":"<p>Get number of messages in memory.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Count of stored messages.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def __len__(self):\n    \"\"\"Get number of messages in memory.\n\n    Returns:\n        int: Count of stored messages.\n    \"\"\"\n    return len(self.messages)\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.__repr__","title":"<code>__repr__()</code>","text":"<p>Get string representation of memory state.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>String showing message count and token usage/limit.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def __repr__(self):\n    \"\"\"Get string representation of memory state.\n\n    Returns:\n        str: String showing message count and token usage/limit.\n    \"\"\"\n    return f\"&lt;STM messages={len(self.messages)} tokens={self.total_tokens()}/{self.max_tokens}&gt;\"\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.append","title":"<code>append(message)</code>","text":"<p>Add a message to memory and evict old messages if needed.</p> <p>If the message doesn't have an ID, generates a UUID for it. After adding the message, ensures token limit compliance by evicting old messages if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The message to add to memory.</p> required Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def append(self, message: Message) -&gt; None:\n    \"\"\"Add a message to memory and evict old messages if needed.\n\n    If the message doesn't have an ID, generates a UUID for it. After\n    adding the message, ensures token limit compliance by evicting old\n    messages if necessary.\n\n    Args:\n        message (Message): The message to add to memory.\n    \"\"\"\n    if not message.id:\n        message.id = str(uuid.uuid4())\n\n    self.messages.append(message)\n    MemoryLogger.log_add(message, destination=\"stm\")\n    self._evict_if_needed()\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.clear","title":"<code>clear()</code>","text":"<p>Remove all messages from memory.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Remove all messages from memory.\"\"\"\n    self.messages = []\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.get_messages","title":"<code>get_messages()</code>","text":"<p>Get all messages currently in memory.</p> <p>Returns:</p> Type Description <code>List[Message]</code> <p>List[Message]: List of all stored messages.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def get_messages(self) -&gt; List[Message]:\n    \"\"\"Get all messages currently in memory.\n\n    Returns:\n        List[Message]: List of all stored messages.\n    \"\"\"\n    return self.messages\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.set_messages","title":"<code>set_messages(messages)</code>","text":"<p>Replace current messages with new list and maintain token limit.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>New messages to store in memory.</p> required Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def set_messages(self, messages: List[Message]) -&gt; None:\n    \"\"\"Replace current messages with new list and maintain token limit.\n\n    Args:\n        messages (List[Message]): New messages to store in memory.\n    \"\"\"\n    self.messages = messages\n    self._evict_if_needed()\n</code></pre>"},{"location":"reference/documentation/core/hippocampus/stm/#neurotrace.core.hippocampus.stm.ShortTermMemory.total_tokens","title":"<code>total_tokens()</code>","text":"<p>Calculate total tokens used by all messages.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Sum of estimated token lengths across all messages.</p> Source code in <code>neurotrace/core/hippocampus/stm.py</code> <pre><code>def total_tokens(self) -&gt; int:\n    \"\"\"Calculate total tokens used by all messages.\n\n    Returns:\n        int: Sum of estimated token lengths across all messages.\n    \"\"\"\n    return sum(m.estimated_token_length() for m in self.messages)\n</code></pre>"},{"location":"reference/documentation/core/tools/factory/","title":"<code>neurotrace.core.tools.factory</code>","text":""},{"location":"reference/documentation/core/tools/factory/#neurotrace.core.tools.factory","title":"<code>neurotrace.core.tools.factory</code>","text":""},{"location":"reference/documentation/core/tools/factory/#neurotrace.core.tools.factory.generic_tool_factory","title":"<code>generic_tool_factory(func, tool_name, tool_description=None, **kwargs)</code>","text":"<p>Creates a LangChain Tool with the given function and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>The function to be wrapped as a tool.</p> required <code>tool_name</code> <code>str</code> <p>Name of the tool.</p> required <code>tool_description</code> <code>str</code> <p>Description of what the tool does. If None, loads description from prompt file. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to Tool constructor.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tool</code> <code>Tool</code> <p>A configured LangChain Tool instance.</p> Source code in <code>neurotrace/core/tools/factory.py</code> <pre><code>def generic_tool_factory(func: callable, tool_name: str, tool_description: str = None, **kwargs) -&gt; Tool:\n    \"\"\"Creates a LangChain Tool with the given function and configuration.\n\n    Args:\n        func (callable): The function to be wrapped as a tool.\n        tool_name (str): Name of the tool.\n        tool_description (str, optional): Description of what the tool does.\n            If None, loads description from prompt file. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to Tool constructor.\n\n    Returns:\n        Tool: A configured LangChain Tool instance.\n    \"\"\"\n    return Tool(name=tool_name, func=func, description=tool_description or load_prompt(tool_name), **kwargs)\n</code></pre>"},{"location":"reference/documentation/core/tools/memory/","title":"<code>neurotrace.core.tools.memory</code>","text":""},{"location":"reference/documentation/core/tools/memory/#neurotrace.core.tools.memory","title":"<code>neurotrace.core.tools.memory</code>","text":""},{"location":"reference/documentation/core/tools/memory/#neurotrace.core.tools.memory.memory_search_tool","title":"<code>memory_search_tool(memory_orchestrator, tool_name='search_memory', tool_description=None, **kwargs)</code>","text":"<p>Creates a tool that searches memory (vector + graph) and returns fused results.</p> <p>Parameters:</p> Name Type Description Default <code>memory_orchestrator</code> <code>MemoryOrchestrator</code> <p>Manages both vector and graph memory.</p> required <code>tool_name</code> <code>str</code> <p>Name of the tool. Defaults to \"search_memory\".</p> <code>'search_memory'</code> <code>tool_description</code> <code>str</code> <p>Description shown to the agent. Loaded from prompt if None.</p> <code>None</code> <code>**kwargs</code> <p>Other Tool configuration options.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tool</code> <code>Tool</code> <p>A LangChain tool for searching across memory.</p> Source code in <code>neurotrace/core/tools/memory.py</code> <pre><code>def memory_search_tool(\n    memory_orchestrator: MemoryOrchestrator,\n    tool_name: str = \"search_memory\",\n    tool_description: str = None,\n    **kwargs,\n) -&gt; Tool:\n    \"\"\"\n    Creates a tool that searches memory (vector + graph) and returns fused results.\n\n    Args:\n        memory_orchestrator (MemoryOrchestrator): Manages both vector and graph memory.\n        tool_name (str): Name of the tool. Defaults to \"search_memory\".\n        tool_description (str): Description shown to the agent. Loaded from prompt if None.\n        **kwargs: Other Tool configuration options.\n\n    Returns:\n        Tool: A LangChain tool for searching across memory.\n    \"\"\"\n\n    def _search(query: str) -&gt; str:\n        \"\"\"\n        Searches both vector and graph memory for relevant info.\n\n        Args:\n            query (str): The question or search query.\n\n        Returns:\n            str: Combined result from vector and graph memory.\n        \"\"\"\n\n        # Vector memory: semantic search\n        vector_results = memory_orchestrator.search_vector_memory(query)\n        vector_summary = (\n            \"\\n\".join(f\"- {doc.content}\" for doc in vector_results)\n            if vector_results\n            else \"No relevant vector memory found.\"\n        )\n\n        # Graph memory: entity/triplet reasoning\n        graph_result = memory_orchestrator.search_graph_memory(query)\n        graph_summary = graph_result if graph_result else \"No graph relationships found.\"\n\n        summarised_memory_context = perform_summarisation(\n            llm=memory_orchestrator.llm,\n            prompt=PROMPT_SUMMARISE_VECTOR_AND_GRAPH_MEMORY,\n            prompt_placeholders={\n                \"vector_memory\": vector_summary,\n                \"graph_memory\": graph_summary,\n            },\n        )\n\n        # Combine and return\n        return (\n            \"This is the summarised context from both vector and graph memory:\\n\"\n            \"--- START OF MEMORY CONTEXT ---\\n\\n\"\n            f\"{summarised_memory_context}\\n\\n\"\n            \"--- END OF MEMORY CONTEXT ---\"\n        )\n\n    return generic_tool_factory(\n        func=_search,\n        tool_name=tool_name,\n        tool_description=tool_description or load_prompt(tool_name),\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/documentation/core/tools/memory/#neurotrace.core.tools.memory.save_memory_tool","title":"<code>save_memory_tool(memory_orchestrator, tool_name='save_memory', tool_description=None, **kwargs)</code>","text":"<p>Creates a tool that allows the agent to explicitly save important memories to long-term vector memory.</p> <p>Parameters:</p> Name Type Description Default <code>vector_memory_adapter</code> <p>The adapter to store messages.</p> required <code>tool_name</code> <code>str</code> <p>Name of the tool. Defaults to \"save_memory\".</p> <code>'save_memory'</code> <code>tool_description</code> <code>str</code> <p>Description of the tool. Loads from prompt if None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword args for Tool.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tool</code> <code>Tool</code> <p>A configured LangChain Tool instance for saving memory.</p> <code>Tool</code> <p>param tool_name:</p> <code>Tool</code> <p>param memory_orchestrator:</p> Source code in <code>neurotrace/core/tools/memory.py</code> <pre><code>def save_memory_tool(\n    memory_orchestrator: MemoryOrchestrator,\n    tool_name: str = \"save_memory\",\n    tool_description: str = None,\n    **kwargs,\n) -&gt; Tool:\n    \"\"\"\n    Creates a tool that allows the agent to explicitly save important memories\n    to long-term vector memory.\n\n    Args:\n        vector_memory_adapter: The adapter to store messages.\n        tool_name (str, optional): Name of the tool. Defaults to \"save_memory\".\n        tool_description (str, optional): Description of the tool. Loads from prompt if None.\n        **kwargs: Additional keyword args for Tool.\n\n    Returns:\n        Tool: A configured LangChain Tool instance for saving memory.\n        :param tool_name:\n        :param memory_orchestrator:\n    \"\"\"\n\n    def _save(summary: str) -&gt; str:\n        \"\"\"\n        Saves a summary in vector memory using the orchestrator.\n\n        Args:\n            summary (str): The summary to save.\n\n        Returns:\n            str: Confirmation message indicating the summary was saved.\n        \"\"\"\n        message_text, convo_tags, *_ = summary.split(\"-- tags:\") + [None, None]\n        if convo_tags:\n            convo_tags = convo_tags.strip().split(\",\")\n        else:\n            convo_tags = []\n\n        message_text = message_text.strip()\n\n        _save_in_vector_memory(memory_orchestrator, message_text, convo_tags)\n        _save_in_graph_memory(memory_orchestrator, message_text, convo_tags)\n        return \"Memory saved in both vector and graph memory.\"\n\n    return generic_tool_factory(\n        func=_save,\n        tool_name=tool_name,\n        tool_description=tool_description or load_prompt(tool_name),\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/documentation/core/tools/system/","title":"<code>neurotrace.core.tools.system</code>","text":""},{"location":"reference/documentation/core/tools/system/#neurotrace.core.tools.system","title":"<code>neurotrace.core.tools.system</code>","text":""},{"location":"reference/documentation/core/tools/system/#neurotrace.core.tools.system.get_current_datetime","title":"<code>get_current_datetime(_='')</code>","text":"<p>Use this tool to get the current date and time in a human-readable format. :param _: :return:</p> Source code in <code>neurotrace/core/tools/system.py</code> <pre><code>def get_current_datetime(_: str = \"\") -&gt; str:\n    \"\"\"\n    Use this tool to get the current date and time in a human-readable format.\n    :param _:\n    :return:\n    \"\"\"\n    return datetime.now().strftime(\"%A, %B %d, %Y at %I:%M %p\")\n</code></pre>"},{"location":"reference/documentation/core/tools/system/#neurotrace.core.tools.system.get_current_location","title":"<code>get_current_location(_='')</code>","text":"<p>Use this tool to get the current location based on the public IP address. :param _: :return:</p> Source code in <code>neurotrace/core/tools/system.py</code> <pre><code>def get_current_location(_: str = \"\") -&gt; str:\n    \"\"\"\n    Use this tool to get the current location based on the public IP address.\n    :param _:\n    :return:\n    \"\"\"\n    try:\n        ip = requests.get(\"https://api.ipify.org\").text\n        response = requests.get(f\"https://ipinfo.io/{ip}/json\").json()\n        city = response.get(\"city\", \"\")\n        region = response.get(\"region\", \"\")\n        country = response.get(\"country\", \"\")\n        return f\"{city}, {region}, {country}\".strip(\", \")\n    except Exception:\n        return \"Unable to determine location.\"\n</code></pre>"},{"location":"reference/documentation/core/tools/system/#neurotrace.core.tools.system.get_day_of_week","title":"<code>get_day_of_week(_='')</code>","text":"<p>Use this tool to get the current day of the week. :param _: :return:</p> Source code in <code>neurotrace/core/tools/system.py</code> <pre><code>def get_day_of_week(_: str = \"\") -&gt; str:\n    \"\"\"\n    Use this tool to get the current day of the week.\n    :param _:\n    :return:\n    \"\"\"\n    return datetime.now().strftime(\"%A\")\n</code></pre>"},{"location":"reference/documentation/core/tools/system/#neurotrace.core.tools.system.get_device_info","title":"<code>get_device_info(_='')</code>","text":"<p>Use this tool to get basic information about the current device. :param _: :return:</p> Source code in <code>neurotrace/core/tools/system.py</code> <pre><code>def get_device_info(_: str = \"\") -&gt; str:\n    \"\"\"\n    Use this tool to get basic information about the current device.\n    :param _:\n    :return:\n    \"\"\"\n    return (\n        f\"System: {platform.system()}\\n\"\n        f\"Release: {platform.release()}\\n\"\n        f\"Processor: {platform.processor()}\\n\"\n        f\"Machine: {platform.machine()}\"\n    )\n</code></pre>"},{"location":"reference/documentation/core/tools/system/#neurotrace.core.tools.system.get_ip_address","title":"<code>get_ip_address(_='')</code>","text":"<p>Use this tool to get the current public IP address. :param _: :return:</p> Source code in <code>neurotrace/core/tools/system.py</code> <pre><code>def get_ip_address(_: str = \"\") -&gt; str:\n    \"\"\"\n    Use this tool to get the current public IP address.\n    :param _:\n    :return:\n    \"\"\"\n    try:\n        return requests.get(\"https://api.ipify.org\").text\n    except Exception:\n        return \"Unable to fetch IP address.\"\n</code></pre>"},{"location":"reference/documentation/core/tools/vector/","title":"<code>neurotrace.core.tools.vector</code>","text":""},{"location":"reference/documentation/core/tools/vector/#neurotrace.core.tools.vector","title":"<code>neurotrace.core.tools.vector</code>","text":""},{"location":"reference/documentation/core/tools/vector/#neurotrace.core.tools.vector.vector_memory_search_tool","title":"<code>vector_memory_search_tool(vector_memory_adapter, tool_name='search_memory', tool_description=None, **kwargs)</code>","text":"<p>Creates a search tool for vector memory using the provided adapter.</p> <p>This factory function creates a LangChain Tool that wraps the search functionality of a vector memory adapter. The tool can be used to perform similarity searches in the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>vector_memory_adapter</code> <code>BaseVectorMemoryAdapter</code> <p>The adapter instance that provides vector memory search functionality.</p> required <code>tool_name</code> <code>str</code> <p>Name of the search tool. Defaults to \"search_memory\".</p> <code>'search_memory'</code> <code>tool_description</code> <code>str</code> <p>Description of what the search tool does. If None, loads description from prompt file. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to Tool constructor.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tool</code> <code>Tool</code> <p>A configured LangChain Tool instance for vector memory search.</p> Source code in <code>neurotrace/core/tools/vector.py</code> <pre><code>def vector_memory_search_tool(\n    vector_memory_adapter: \"BaseVectorMemoryAdapter\",\n    tool_name: str = \"search_memory\",\n    tool_description: str = None,\n    **kwargs,\n) -&gt; Tool:\n    \"\"\"Creates a search tool for vector memory using the provided adapter.\n\n    This factory function creates a LangChain Tool that wraps the search functionality\n    of a vector memory adapter. The tool can be used to perform similarity searches\n    in the vector store.\n\n    Args:\n        vector_memory_adapter (BaseVectorMemoryAdapter): The adapter instance that\n            provides vector memory search functionality.\n        tool_name (str, optional): Name of the search tool. Defaults to \"search_memory\".\n        tool_description (str, optional): Description of what the search tool does.\n            If None, loads description from prompt file. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to Tool constructor.\n\n    Returns:\n        Tool: A configured LangChain Tool instance for vector memory search.\n    \"\"\"\n    return generic_tool_factory(\n        func=vector_memory_adapter.search,\n        tool_name=tool_name,\n        tool_description=tool_description or load_prompt(tool_name),\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/documentation/neurotrace_logging/logger_factory/","title":"<code>neurotrace.neurotrace_logging.logger_factory</code>","text":""},{"location":"reference/documentation/neurotrace_logging/logger_factory/#neurotrace.neurotrace_logging.logger_factory","title":"<code>neurotrace.neurotrace_logging.logger_factory</code>","text":""},{"location":"reference/documentation/neurotrace_logging/logger_factory/#neurotrace.neurotrace_logging.logger_factory.get_logger","title":"<code>get_logger(name)</code>","text":"<p>Central logger factory for Neurotrace with support for: - Stream logs - Daily rotating file logs - Optional network logging via sockets</p> Source code in <code>neurotrace/neurotrace_logging/logger_factory.py</code> <pre><code>def get_logger(name: str) -&gt; logging.Logger:\n    \"\"\"\n    Central logger factory for Neurotrace with support for:\n    - Stream logs\n    - Daily rotating file logs\n    - Optional network logging via sockets\n    \"\"\"\n\n    logger = logging.getLogger(name)\n    logger.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))\n    logger.propagate = False  # prevent duplicate logs\n\n    if not logger.handlers:\n        formatter = logging.Formatter(\"[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s\")\n\n        if LOG_TO_STREAM:\n            stream_handler = logging.StreamHandler()\n            stream_handler.setFormatter(formatter)\n            logger.addHandler(stream_handler)\n\n        if LOG_TO_FILE:\n            Path(LOG_FILE_PATH).parent.mkdir(parents=True, exist_ok=True)\n            file_handler = TimedRotatingFileHandler(LOG_FILE_PATH, when=\"midnight\", backupCount=7)\n            file_handler.setFormatter(formatter)\n            logger.addHandler(file_handler)\n\n        if LOG_TO_NETWORK:\n            try:\n                socket_handler = SocketHandler(LOG_HOST, LOG_PORT)\n                socket_handler.setFormatter(formatter)\n                logger.addHandler(socket_handler)\n            except Exception as e:\n                logger.warning(f\"Could not attach network log handler: {e}\")\n\n    return logger\n</code></pre>"},{"location":"reference/documentation/neurotrace_logging/memory_logger/","title":"<code>neurotrace.neurotrace_logging.memory_logger</code>","text":""},{"location":"reference/documentation/neurotrace_logging/memory_logger/#neurotrace.neurotrace_logging.memory_logger","title":"<code>neurotrace.neurotrace_logging.memory_logger</code>","text":""},{"location":"reference/documentation/prompts/task_prompts/","title":"<code>neurotrace.prompts.task_prompts</code>","text":""},{"location":"reference/documentation/prompts/task_prompts/#neurotrace.prompts.task_prompts","title":"<code>neurotrace.prompts.task_prompts</code>","text":""}]}